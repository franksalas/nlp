{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus: 7 documents\n",
      "Count Vectorizer:\n",
      "\n",
      "{'convert': 10, 'collection': 7, 'of': 49, 'text': 69, 'documents': 14, 'to': 73, 'matrix': 40, 'token': 74, 'occurrences': 48, 'it': 34, 'turns': 76, 'into': 32, 'scipy': 60, 'sparse': 62, 'holding': 24, 'occurrence': 47, 'counts': 11, 'or': 51, 'binary': 5, 'information': 30, 'possibly': 57, 'normalized': 46, 'as': 2, 'frequencies': 21, 'if': 26, 'norm': 45, 'l1': 35, 'projected': 58, 'on': 50, 'the': 70, 'euclidean': 16, 'unit': 78, 'sphere': 63, 'l2': 36, 'this': 72, 'vectorizer': 81, 'implementation': 27, 'uses': 80, 'hashing': 23, 'trick': 75, 'find': 19, 'string': 68, 'name': 42, 'feature': 18, 'integer': 31, 'index': 29, 'mapping': 39, 'strategy': 66, 'has': 22, 'several': 61, 'advantages': 0, 'is': 33, 'very': 82, 'low': 38, 'memory': 41, 'scalable': 59, 'large': 37, 'datasets': 12, 'there': 71, 'no': 44, 'need': 43, 'store': 65, 'vocabulary': 83, 'dictionary': 13, 'in': 28, 'fast': 17, 'pickle': 55, 'and': 1, 'un': 77, 'holds': 25, 'state': 64, 'besides': 4, 'constructor': 9, 'parameters': 53, 'can': 6, 'be': 3, 'used': 79, 'streaming': 67, 'partial': 54, 'fit': 20, 'parallel': 52, 'pipeline': 56, 'computed': 8, 'during': 15}\n",
      "Resulting matrix has 7 data points and 84 features.\n",
      "\n",
      "Document 1: \n",
      "[[0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 1 1 0 0 0 0 0 0 0 0 0]]\n",
      "Hashing Vectorizer:\n",
      "\n",
      "Resulting matrix has 7 data points and 1048576 features.\n",
      "\n",
      "Document 1: \n",
      "  (0, 22468)\t1.0\n",
      "  (0, 124863)\t1.0\n",
      "  (0, 164975)\t1.0\n",
      "  (0, 174171)\t1.0\n",
      "  (0, 264705)\t1.0\n",
      "  (0, 479532)\t2.0\n",
      "  (0, 548700)\t1.0\n",
      "  (0, 676585)\t1.0\n",
      "  (0, 741852)\t1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "corpus = [\n",
    "    'Convert a collection of text documents to a matrix of token occurrences',\n",
    "    'It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.',\n",
    "    'This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.',\n",
    "    'This strategy has several advantages:',\n",
    "    'it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory',\n",
    "    'it is fast to pickle and un-pickle as it holds no state besides the constructor parameters',\n",
    "    'it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.'\n",
    "]\n",
    "\n",
    "print('Processing corpus: {} documents'.format(len(corpus)))\n",
    "\n",
    "print('Count Vectorizer:\\n')\n",
    "vectorizer = feature_extraction.text.CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# Count Vectorizer stores a dictionary: a number per word\n",
    "print(vectorizer.vocabulary_)\n",
    "print('Resulting matrix has {} data points and {} features.\\n'.format(\n",
    "    X.shape[0], X.shape[1]))\n",
    "print('Document 1: \\n{}'.format(X[0].toarray()))\n",
    "# as the number of words increase, you need a bigger and bigger dictionary!\n",
    "\n",
    "\n",
    "print('Hashing Vectorizer:\\n')\n",
    "\n",
    "# norm=None means we don't normalize the values\n",
    "# alternative_sign=False means that we don't alternate the value's signs to\n",
    "#   conserve any mathematical properties\n",
    "vectorizer = feature_extraction.text.HashingVectorizer(\n",
    "    norm=None, alternate_sign=False)\n",
    "X = vectorizer.transform(corpus)  # not fit_transform\n",
    "\n",
    "print('Resulting matrix has {} data points and {} features.\\n'.format(\n",
    "    X.shape[0], X.shape[1]))\n",
    "\n",
    "# > Resulting matrix has 7 data points and 1048576 features.\n",
    "\n",
    "print('Document 1: \\n{}'.format(X[0]))\n",
    "\n",
    "# Document 1: \n",
    "#   (0, 22468)\t0.2886751345948129\n",
    "#   (0, 124863)\t-0.2886751345948129\n",
    "#   (0, 164975)\t-0.2886751345948129\n",
    "#   (0, 174171)\t0.2886751345948129\n",
    "#   (0, 264705)\t0.2886751345948129\n",
    "#   (0, 479532)\t0.5773502691896258\n",
    "#   (0, 548700)\t-0.2886751345948129\n",
    "#   (0, 676585)\t-0.2886751345948129\n",
    "#   (0, 741852)\t-0.2886751345948129\n",
    "#   Read the above as:\n",
    "#   (document_index, feature_index) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
