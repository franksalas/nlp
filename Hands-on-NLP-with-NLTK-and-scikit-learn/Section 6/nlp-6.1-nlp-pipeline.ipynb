{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    \"\"\"\n",
    "    This strategy has several advantages:\n",
    "    it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    "    it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
    "    it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), \n",
    "    possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(f):\n",
    "    '''pipeline decorator that calls next() on function f()'''\n",
    "    def start_pipeline(*args, **kwargs):\n",
    "        nf = f(*args, **kwargs)\n",
    "        next(nf)\n",
    "        return nf\n",
    "    return start_pipeline\n",
    "\n",
    "\n",
    "def ingest(corpus, targets):\n",
    "    for text in corpus:\n",
    "        for t in targets:\n",
    "            t.send(text)\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def tokenize_sentences(targets):\n",
    "    while True:\n",
    "        text = (yield)  # (yield) gets an item from an upstream step\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            for target in targets:\n",
    "                target.send(sentence)  # send() sends data downstream\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def tokenize_words(targets):\n",
    "    while True:\n",
    "        sentence = (yield)\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        for target in targets:\n",
    "            target.send(words)\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def pos_tagging(targets):\n",
    "    while True:\n",
    "        words = (yield)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "        for target in targets:\n",
    "            target.send(tagged_words)\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def ne_chunking(targets):\n",
    "    while True:\n",
    "        tagged_words = (yield)\n",
    "        ner_tagged = nltk.ne_chunk(tagged_words)\n",
    "        for target in targets:\n",
    "            target.send(ner_tagged)\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def printline(title):\n",
    "    while True:\n",
    "        line = (yield)\n",
    "        print(title)\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens:\n",
      "['This', 'strategy', 'has', 'several', 'advantages', ':', 'it', 'is', 'very', 'low', 'memory', 'scalable', 'to', 'large', 'datasets', 'as', 'there', 'is', 'no', 'need', 'to', 'store', 'a', 'vocabulary', 'dictionary', 'in', 'memory', 'it', 'is', 'fast', 'to', 'pickle', 'and', 'un-pickle', 'as', 'it', 'holds', 'no', 'state', 'besides', 'the', 'constructor', 'parameters', 'it', 'can', 'be', 'used', 'in', 'a', 'streaming', '(', 'partial', 'fit', ')', 'or', 'parallel', 'pipeline', 'as', 'there', 'is', 'no', 'state', 'computed', 'during', 'fit', '.']\n",
      "Results:\n",
      "(S\n",
      "  This/DT\n",
      "  strategy/NN\n",
      "  has/VBZ\n",
      "  several/JJ\n",
      "  advantages/NNS\n",
      "  :/:\n",
      "  it/PRP\n",
      "  is/VBZ\n",
      "  very/RB\n",
      "  low/JJ\n",
      "  memory/NN\n",
      "  scalable/NN\n",
      "  to/TO\n",
      "  large/JJ\n",
      "  datasets/NNS\n",
      "  as/IN\n",
      "  there/EX\n",
      "  is/VBZ\n",
      "  no/DT\n",
      "  need/NN\n",
      "  to/TO\n",
      "  store/VB\n",
      "  a/DT\n",
      "  vocabulary/JJ\n",
      "  dictionary/NN\n",
      "  in/IN\n",
      "  memory/NN\n",
      "  it/PRP\n",
      "  is/VBZ\n",
      "  fast/JJ\n",
      "  to/TO\n",
      "  pickle/VB\n",
      "  and/CC\n",
      "  un-pickle/JJ\n",
      "  as/IN\n",
      "  it/PRP\n",
      "  holds/VBZ\n",
      "  no/DT\n",
      "  state/NN\n",
      "  besides/IN\n",
      "  the/DT\n",
      "  constructor/NN\n",
      "  parameters/NNS\n",
      "  it/PRP\n",
      "  can/MD\n",
      "  be/VB\n",
      "  used/VBN\n",
      "  in/IN\n",
      "  a/DT\n",
      "  streaming/NN\n",
      "  (/(\n",
      "  partial/JJ\n",
      "  fit/NN\n",
      "  )/)\n",
      "  or/CC\n",
      "  parallel/JJ\n",
      "  pipeline/NN\n",
      "  as/IN\n",
      "  there/EX\n",
      "  is/VBZ\n",
      "  no/DT\n",
      "  state/NN\n",
      "  computed/VBD\n",
      "  during/IN\n",
      "  fit/NN\n",
      "  ./.)\n",
      "Word tokens:\n",
      "['It', 'turns', 'a', 'collection', 'of', 'text', 'documents', 'into', 'a', 'scipy.sparse', 'matrix', 'holding', 'token', 'occurrence', 'counts', '(', 'or', 'binary', 'occurrence', 'information', ')', ',', 'possibly', 'normalized', 'as', 'token', 'frequencies', 'if', 'norm=', '’', 'l1', '’', 'or', 'projected', 'on', 'the', 'euclidean', 'unit', 'sphere', 'if', 'norm=', '’', 'l2', '’', '.']\n",
      "Results:\n",
      "(S\n",
      "  It/PRP\n",
      "  turns/VBZ\n",
      "  a/DT\n",
      "  collection/NN\n",
      "  of/IN\n",
      "  text/JJ\n",
      "  documents/NNS\n",
      "  into/IN\n",
      "  a/DT\n",
      "  scipy.sparse/JJ\n",
      "  matrix/NN\n",
      "  holding/VBG\n",
      "  token/JJ\n",
      "  occurrence/NN\n",
      "  counts/NNS\n",
      "  (/(\n",
      "  or/CC\n",
      "  binary/JJ\n",
      "  occurrence/NN\n",
      "  information/NN\n",
      "  )/)\n",
      "  ,/,\n",
      "  possibly/RB\n",
      "  normalized/VBN\n",
      "  as/IN\n",
      "  token/JJ\n",
      "  frequencies/NNS\n",
      "  if/IN\n",
      "  norm=/JJ\n",
      "  ’/NNP\n",
      "  l1/NN\n",
      "  ’/NN\n",
      "  or/CC\n",
      "  projected/VBN\n",
      "  on/IN\n",
      "  the/DT\n",
      "  euclidean/JJ\n",
      "  unit/NN\n",
      "  sphere/RB\n",
      "  if/IN\n",
      "  norm=/JJ\n",
      "  ’/NNP\n",
      "  l2/NN\n",
      "  ’/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ingest(corpus, [\n",
    "    tokenize_sentences([\n",
    "        tokenize_words([\n",
    "            printline('Word tokens:'),\n",
    "            pos_tagging([\n",
    "                ne_chunking([\n",
    "                    printline('Results:')\n",
    "                ])\n",
    "            ])\n",
    "        ])\n",
    "    ])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
