{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Sentiment Analysis on movie reviews\n",
    "- Write a text classification pipeline to classify movie reviews as either positive or negative.\n",
    "- Find a good set of parameters using grid search.\n",
    "- Evaluate the performance on a held out test set\n",
    "\n",
    "\n",
    "## Build a sentiment analysis / polarity model\n",
    "\n",
    "Sentiment analysis can be casted as a binary text classification problem,\n",
    "that is fitting a linear classifier on features extracted from the text\n",
    "of the user messages so as to guess wether the opinion of the author is\n",
    "positive or negative.\n",
    "\n",
    "In this examples we will use a movie review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch movie_reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mfetch_data.py\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls data/movie_reviews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 15.6 ms, total: 15.6 ms\n",
      "Wall time: 311 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! python data/movie_reviews/fetch_data.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;36;40mneg\u001b[0m/  \u001b[01;36;40mpos\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls data/movie_reviews/txt_sentoken/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;36;40mneg\u001b[0m/  \u001b[01;36;40mpos\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls data/movie_reviews/txt_sentoken/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "# the training data folder must be passed as first argument\n",
    "movie_reviews_data_folder = 'data/movie_reviews/txt_sentoken/'\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK: \n",
    "## Build a vectorizer / classifier pipeline that filters out tokens that are too rare or too frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "    ('clf', LinearSVC(C=1000)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK: \n",
    "\n",
    "## Build a grid search to find out whether unigrams or bigrams are more useful.\n",
    "\n",
    "### Fit the pipeline on the training set using grid search for the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK: \n",
    "\n",
    "### print the mean and std for each candidate along with the parameter settings for all the candidates explored by grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.83; std - 0.01\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.84; std - 0.02\n"
     ]
    }
   ],
   "source": [
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK: \n",
    "\n",
    "## Predict the outcome on the testing set and store it in a variable named `y_predicted`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = grid_search.predict(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Print the classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.87      0.86      0.86       244\n",
      "        pos       0.87      0.88      0.87       256\n",
      "\n",
      "avg / total       0.87      0.87      0.87       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display and print and plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[209  35]\n",
      " [ 31 225]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
