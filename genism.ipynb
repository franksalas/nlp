{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim – Vectorizing Text and Transformations and n-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['dog', 'sat', 'mat', 'love', 'cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TF-IDF is short for term frequency-inverse document frequency. Largely used in search engines to find relevant documents based on a query, it is a rather intuitive approach to converting our sentences into vectors.\n",
    "\n",
    "```\n",
    "TF(t) = (number of times term t appears in a document) / (total number of terms in the document)\n",
    "\n",
    "IDF(t) = log_e (total number of documents / number of documents with term t in it)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector transformation in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b8169bf49556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mu\"Football club Arsenal defeat local rivals this weekend.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"Weekend football frenzy takes over London.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"Bank open for takeover bids after losing millions.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"London football clubs bid to move to Wembley stadium.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"Arsenal bid 50 million pounds for striker Kane.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"Financial troubles result in loss of millions for bank.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"Western bank files for bankruptcy after financial losses.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"London football club is taken over by oil millionaire from Russia.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"Banking on finances not working for Russia.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "documents = [u\"Football club Arsenal defeat local rivals this weekend.\", u\"Weekend football frenzy takes over London.\", u\"Bank open for takeover bids after losing millions.\", u\"London football clubs bid to move to Wembley stadium.\", u\"Arsenal bid 50 million pounds for striker Kane.\", u\"Financial troubles result in loss of millions for bank.\", u\"Western bank files for bankruptcy after financial losses.\", u\"London football club is taken over by oil millionaire from Russia.\", u\"Banking on finances not working for Russia.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Football club Arsenal defeat local rivals this weekend.',\n",
       " 'Weekend football frenzy takes over London.',\n",
       " 'Bank open for takeover bids after losing millions.',\n",
       " 'London football clubs bid to move to Wembley stadium.',\n",
       " 'Arsenal bid 50 million pounds for striker Kane.',\n",
       " 'Financial troubles result in loss of millions for bank.',\n",
       " 'Western bank files for bankruptcy after financial losses.',\n",
       " 'London football club is taken over by oil millionaire from Russia.',\n",
       " 'Banking on finances not working for Russia.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "texts = []\n",
    "for document in documents:\n",
    "    text = []\n",
    "    doc = nlp(document)\n",
    "    for w in doc:\n",
    "        if not w.is_stop and not w.is_punct and not w.like_num:\n",
    "            text.append(w.lemma_)\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'],\n",
       " ['weekend', 'football', 'frenzy', 'take', 'london'],\n",
       " ['bank', 'open', 'takeover', 'bid', 'lose', 'million'],\n",
       " ['london', 'football', 'club', 'bid', 'wembley', 'stadium'],\n",
       " ['arsenal', 'bid', 'pound', 'striker', 'kane'],\n",
       " ['financial', 'trouble', 'result', 'loss', 'million', 'bank'],\n",
       " ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'],\n",
       " ['london', 'football', 'club', 'take', 'oil', 'millionaire', 'russia'],\n",
       " ['bank', 'finance', 'work', 'russia']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start by setting up a bagof word representation for our mini-corupus. Gensim allows us to do this very conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arsenal': 0, 'club': 1, 'defeat': 2, 'football': 3, 'local': 4, 'rival': 5, 'weekend': 6, 'frenzy': 7, 'london': 8, 'take': 9, 'bank': 10, 'bid': 11, 'lose': 12, 'million': 13, 'open': 14, 'takeover': 15, 'stadium': 16, 'wembley': 17, 'kane': 18, 'pound': 19, 'striker': 20, 'financial': 21, 'loss': 22, 'result': 23, 'trouble': 24, 'bankruptcy': 25, 'file': 26, 'western': 27, 'millionaire': 28, 'oil': 29, 'russia': 30, 'finance': 31, 'work': 32}\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 32 unique words in our corpus, all of which are represented in our dictionary with each word being assigned an index value. When we refer to a word's word_id henceforth, it means we are talking about the words integer-id mapping made by the dictionary.We will be using the doc2bow method, which, as the name suggests, helps convert our document to bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(1, 1), (3, 1), (8, 1), (11, 1), (16, 1), (17, 1)], [(0, 1), (11, 1), (18, 1), (19, 1), (20, 1)], [(10, 1), (13, 1), (21, 1), (22, 1), (23, 1), (24, 1)], [(10, 1), (21, 1), (22, 1), (25, 1), (26, 1), (27, 1)], [(1, 1), (3, 1), (8, 1), (9, 1), (28, 1), (29, 1), (30, 1)], [(10, 1), (30, 1), (31, 1), (32, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "his is a list of lists, where each individual list represents a documents bag-of-words representation. A reminder: you might see different numbers in your list, this is because each time you create a dictionary, different mappings will occur. Unlike the example we demonstrated, where an absence of a word was a 0, we use tuples that represent (word_id, word_count). We can easily verify this by checking the original sentence, mapping each word to its integer ID and reconstructing our list. We can also notice in this case each document has not greater than one count of each word - in smaller corpuses, this tends to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by storing the corpus, once it is created, to disk. One way to do this is as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "corpora.MmCorpus.serialize('/tmp/example.mm', corpus)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what does a TF-IDF representation of our corpus look like? All we have to do is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3292179861221233), (1, 0.24046829370585296), (2, 0.4809365874117059), (3, 0.1774993848325406), (4, 0.4809365874117059), (5, 0.4809365874117059), (6, 0.3292179861221233)]\n",
      "[(3, 0.24212967666975266), (6, 0.4490913847888623), (7, 0.6560530929079719), (8, 0.32802654645398593), (9, 0.4490913847888623)]\n",
      "[(10, 0.18797844084016113), (11, 0.25466485399352906), (12, 0.5093297079870581), (13, 0.3486540744136096), (14, 0.5093297079870581), (15, 0.5093297079870581)]\n",
      "[(1, 0.29431054749542984), (3, 0.21724253258131512), (8, 0.29431054749542984), (11, 0.29431054749542984), (16, 0.5886210949908597), (17, 0.5886210949908597)]\n",
      "[(0, 0.354982288765831), (11, 0.25928712547209604), (18, 0.5185742509441921), (19, 0.5185742509441921), (20, 0.5185742509441921)]\n",
      "[(10, 0.19610384738673725), (13, 0.3637247180792822), (21, 0.3637247180792822), (22, 0.3637247180792822), (23, 0.5313455887718271), (24, 0.5313455887718271)]\n",
      "[(10, 0.18286519950508276), (21, 0.3391702611796705), (22, 0.3391702611796705), (25, 0.4954753228542582), (26, 0.4954753228542582), (27, 0.4954753228542582)]\n",
      "[(1, 0.2645025265769199), (3, 0.1952400253294319), (8, 0.2645025265769199), (9, 0.3621225392416359), (28, 0.5290050531538398), (29, 0.5290050531538398), (30, 0.3621225392416359)]\n",
      "[(10, 0.22867660961662029), (30, 0.4241392327204109), (31, 0.6196018558242014), (32, 0.6196018558242014)]\n"
     ]
    }
   ],
   "source": [
    "for document in tfidf[corpus]:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember what we said about TF-IDF, you will be able to identify the float next to each word_id - it is the product of the TF and IDF scores for that particular word, instead of just the word count which was present before. The higher the score, the more important the word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams and some more preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "bigram = gensim.models.Phrases(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "texts = [bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
