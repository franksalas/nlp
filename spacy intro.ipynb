{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic preprocessing with language models\n",
    "```python\n",
    "doc = nlp('this is a sentence.')\n",
    "```\n",
    "\n",
    "When you call nlp on Unicode text, spaCy first tokenizes the text to produce a Doc object. Doc is then processed in several different steps, what we also refer to as our pipeline.\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/b5eDIoW.png)\n",
    "\n",
    "\n",
    "## Tokenising text\n",
    "\n",
    "\n",
    "Different languages will have different tokenization rules. Let's look at an example of how tokenization might work in English. For the sentence – Let us go to the park., it's quite straightforward, and would be broken up as follows, with the appropriate numerical indices:\n",
    "\n",
    "![](https://i.imgur.com/ZfABw3t.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/4rPNwm1.png)\n",
    "\n",
    "\n",
    "## Part of speach (POS) tagging\n",
    "\n",
    "\n",
    "The second component of the default pipeline we described before was the tensorizer.\n",
    "\n",
    "A tensorizer encodes the internal representation of the doc as an array of floats. This is a necessary step because spaCy's models are neural network models, and only speak tensors – every Doc object is expected to be tenzorised. We as users do not need to concern ourselves with this. After this step, we start with our first annotation – part of speech tagging.\n",
    "\n",
    "In the first chapter, we briefly mentioned POS-tagging as marking each token of the sentence with its appropriate part of speech, such as noun, verb, and so on. spaCy uses a statistical model to perform its POS-tagging. To get the annotation from a token, we simply look up the pos_ attribute on the token.\n",
    "\n",
    "Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2d0454b479d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'John and I went to the park'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp('John and I went to the park')\n",
    "\n",
    "for token in doc:\n",
    "  print((token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name entity recognition\n",
    "\n",
    "We now have the last part of our pipeline, where we perform named entity recognition. A named entity is a real-world object that is assigned a name – for example, a person, a country, a product, or organization. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. We have to remember that since models are statistical and depend on the examples they were trained on, they don't always work perfectly and might need some tuning later, depending on your use case – we have a chapter saved up just to better understand named entity recognition and how to train our own models.Named entities are available as the ents property of a Doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft 0 9 ORG\n",
      "Europe 31 37 LOC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Microsoft has offices all over Europe.')\n",
    "for ent in doc.ents:  \n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has the following built-in entity types:\n",
    "\n",
    "- PERSON: People, including fictional ones\n",
    "- NORP: Nationalities or religious or political groups\n",
    "- FACILITY: Buildings, airports, highways, bridges, and so on\n",
    "- ORG: Companies, agencies, institutions, and so on\n",
    "- GPE: Countries, cities, and states\n",
    "- LOC: Non GPE locations, mountain ranges, and bodies of water\n",
    "- PRODUCT: Objects, vehicles, foods, and so on (not services)\n",
    "- EVENT: Named hurricanes, battles, wars, sports events, and so on\n",
    "- WORK_OF_ART: Titles of books, songs, and so onLAW: Named documents made into laws\n",
    "- LANGUAGE: Any named language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule based matching \n",
    "\n",
    "\n",
    "- ORTH: The exact verbatim text of a token\n",
    "- LOWER, UPPER: The lowercase and uppercase form of the token\n",
    "- IS_ALPHA: Token text consists of alphanumeric chars\n",
    "- IS_ASCII: Token text consists of ASCII characters\n",
    "- IS_DIGIT: Token text consists of digits\n",
    "- IS_LOWER, IS_UPPER, IS_TITLE: Token text is in lowercase, uppercase, and title\n",
    "- IS_PUNCT, IS_SPACE, IS_STOP: Token is punctuation, whitespace, and a stop word\n",
    "- LIKE_NUM, LIKE_URL, LIKE_EMAIL: Token text resembles a number, URL, and email\n",
    "- POS, TAG: The token's simple and extended POS tag\n",
    "- DEP, LEMMA, SHAPE: The token's dependency label, lemma, and shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PReprocessing\n",
    "\n",
    "The wonderful thing about preprocessing text is that it almost feels intuitive – we get rid of any information which we think won't be used in our final output and keep what we feel is important. Here, our information is words – and some words do not always provide useful insights. In the text mining and natural language processing community, these words are called stop words [22]\n",
    "\n",
    "Stop words are words that are filtered out of our text before we run any text mining or NLP algorithms on it. Again, we would like to draw attention to the fact this is not in every case – if we intend to find stylistic similarities or understand how writers use stop words, we would obviously need to stop words!There is no universal stop words list for each language, and it largely depends on the use case and what kind of results we expect to be seeing. Usually, it is a list of the most common words in the language, such as of, the, want, to, and have.With spaCy, stop words are very easy to identify – each token has an IS_STOP attribute, which lets us know if the word is a stop word or not. The list of all the stop words for each language can be found in the spacy/lang [20] folder.We can also add our own stop words to the list of stop words. For example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we can also add our own stop words to teh list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "my_stop_words = [u'say', u'be', u'said', u'says', u'saying', 'field']\n",
    "for stopword in my_stop_words:  \n",
    "    lexeme = nlp.vocab[stopword]  \n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whom', 'almost', 'so', 'were', 'first', 'ca', 'against', 'each', 'ever', 'herself', 'several', 'something', 'twelve', 'see', 'nobody', 'also', 'but', 'elsewhere', 'keep', 'less', 'enough', 'mine', 'thus', 'namely', 'yourself', 'else', 'twenty', 'around', 'did', 'whoever', 'what', 'whose', 'former', 'fifty', 'used', 'beyond', 'six', 'all', 'being', 'on', 'three', 'nowhere', 'really', 'you', 'within', 'after', 'another', 'just', 'top', 'it', 'anywhere', 'front', 'amount', 'many', 'move', 'name', 'towards', 'together', 'because', 'if', 'thereby', 'have', 'get', 'whole', 'before', 'however', 'my', 'somewhere', 'both', 'whereupon', 'per', 'seemed', 'again', 'an', 'go', 'between', 'and', 'us', 'of', 'her', 'via', 'when', 'no', 'though', 'due', 'fifteen', 'someone', 'besides', 'am', 'never', 'yet', 'why', 'does', 'once', 'along', 'about', 'nevertheless', 'thence', 'or', 'was', 'upon', 'become', 'full', 'own', 'everything', 'from', 'various', 'at', 'whence', 'very', 'cannot', 'becoming', 'every', 'meanwhile', 'its', 'quite', 'them', 'too', 'this', 'using', 'please', 'whither', 'further', 'behind', 'although', 'moreover', 'whatever', 'myself', 'done', 'unless', 'beside', 'be', 'seeming', 'here', 'should', 'seems', 'those', 'are', 'could', 'he', 'either', 'there', 'the', 'themselves', 'thru', 'with', 'yours', 'she', 'latterly', 'until', 'can', 'ourselves', 'five', 'therein', 'others', 'down', 'amongst', 'third', 'say', 'otherwise', 'side', 'show', 'thereupon', 'often', 'other', 'perhaps', 'wherever', 'off', 'for', 'seem', 'anyone', 'his', 'last', 'under', 'sometime', 'through', 'part', 'least', 'in', 'noone', 'which', 'been', 'none', 'call', 'whereafter', 'their', 'alone', 'regarding', 'anything', 'below', 'whereby', 'same', 'that', 'wherein', 'himself', 'whenever', 'above', 'eleven', 'forty', 'me', 'would', 'much', 'across', 'some', 'mostly', 'always', 'empty', 'somehow', 'two', 'indeed', 'hundred', 'since', 'may', 'few', 'had', 'except', 'sixty', 'over', 'sometimes', 'beforehand', 'hence', 'now', 'i', 'to', 'will', 're', 'nine', 'put', 'without', 'doing', 'during', 'already', 'bottom', 'hereafter', 'where', 'your', 'him', 'even', 'a', 'hereupon', 'among', 'might', 'afterwards', 'has', 'as', 'eight', 'nothing', 'one', 'everyone', 'most', 'any', 'these', 'back', 'not', 'is', 'throughout', 'ours', 'nor', 'became', 'than', 'made', 'our', 'itself', 'anyhow', 'out', 'hers', 'ten', 'whether', 'everywhere', 'well', 'becomes', 'neither', 'only', 'then', 'rather', 'while', 'serious', 'take', 'formerly', 'make', 'onto', 'they', 'latter', 'yourselves', 'such', 'toward', 'up', 'who', 'by', 'do', 'give', 'still', 'hereby', 'thereafter', 'how', 'we', 'next', 'therefore', 'more', 'into', 'herein', 'must', 'whereas', 'four', 'anyway'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import  STOP_WORDS\n",
    "\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "STOP_WORDS.add('your_additional_stop_words_here')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horse', 'gallop', 'past', 'river']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('the horse galloped down the field and past the river.')\n",
    "sentence = []\n",
    "for w in doc:\n",
    "    # if it's not a stop word or punctuation mark, add it to our article! \n",
    "    if w.text != 'n' and not w.is_stop and not w.is_punct and not w.like_num:    \n",
    "        # we add the lematized version of the word    \n",
    "        sentence.append(w.lemma_)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
