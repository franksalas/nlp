{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Classify Text\n",
    "\n",
    "With the data we gathered in the previous section, we can now build our text\n",
    "analysis service. At the core, the service will use a classifier similar to the one we\n",
    "created earlier, in chapter 2.2. The main difference would be that the input for this\n",
    "classifier won’t be a single word, but an entire block of text.\n",
    "\n",
    "This means we can use words as features and more precisely, we can use word\n",
    "counts as features. We will then train a classifier to decipher the correlations\n",
    "between the word counts for us. The main thing to note here is that categories\n",
    "contain certain words with a certain frequency.\n",
    "\n",
    "### Text Feature Extractor\n",
    "Remember that in the previous chapter we transformed a word, a name to be more\n",
    "specific, into a feature vector. Now, will perform a similar operation on a block of\n",
    "text. Let’s take a random sentence and transform it into the feature space:\n",
    "\n",
    ">How much wood does a woodchuck chuck if a woodchuck could chuck wood\n",
    "\n",
    "This is how this sentence would look transformed into the feature space:\n",
    "\n",
    "**Tongue Twister to Feature Space**\n",
    "\n",
    "```python\n",
    "\n",
    "{\n",
    "'wood': 2,\n",
    "'a': 2,\n",
    "'woodchuck': 2,\n",
    "'chuck': 2,\n",
    "'wow': 1,\n",
    "'much': 1,\n",
    "'does': 1,\n",
    "'if': 1,\n",
    "'could': 1\n",
    "}\n",
    "```\n",
    "\n",
    "Here’s how simple it is to transform it:\n",
    "\n",
    "**Convert Text to Dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'how': 1,\n",
       "         'much': 1,\n",
       "         'wood': 2,\n",
       "         'does': 1,\n",
       "         'a': 2,\n",
       "         'woodchuck': 2,\n",
       "         'chuck': 2,\n",
       "         'if': 1,\n",
       "         'could': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "text = \"\"\"\n",
    "How much wood does a woodchuck chuck\n",
    "if a woodchuck could chuck wood\n",
    "\"\"\"\n",
    "collections.Counter(text.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that might throw you off at this point is that this method doesn’t take\n",
    "into consideration the order of the words inside a text. This is one of the known\n",
    "drawbacks of this method. Using word counts is an approximation and this type of\n",
    "approximation is called **Bag of Words.**\n",
    "\n",
    "\n",
    "There are methods that deal with actual sequences, such as Hidden Markov Models\n",
    "or Recurrent Neutral Networks, but using these type of methods are not the subject\n",
    "of this book.\n",
    "\n",
    "\n",
    "Bag of Words models are really popular and widely used because they are simple\n",
    "and can perform pretty well. We can get even better approximations of the\n",
    "sequence using bigram or trigram models.\n",
    "\n",
    "As we discussed in the first chapters, a bigram is a pair of adjacent words inside a\n",
    "sentence and a trigram is a triplet of such words. Here’s how to compute them for\n",
    "a given text using nltk utils:\n",
    "\n",
    "**Compute Bigram and Trigram Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "text = \"\"\"\n",
    "How much wood does a woodchuck chuck\n",
    "if a woodchuck could chuck wood\n",
    "\"\"\"\n",
    "\n",
    "bigram_features = collections.Counter(\n",
    "    list(nltk.bigrams(text.lower().split())))\n",
    "\n",
    "trigram_features = collections.Counter(\n",
    "    list(nltk.trigrams(text.lower().split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('a', 'woodchuck'): 2,\n",
      "         ('how', 'much'): 1,\n",
      "         ('much', 'wood'): 1,\n",
      "         ('wood', 'does'): 1,\n",
      "         ('does', 'a'): 1,\n",
      "         ('woodchuck', 'chuck'): 1,\n",
      "         ('chuck', 'if'): 1,\n",
      "         ('if', 'a'): 1,\n",
      "         ('woodchuck', 'could'): 1,\n",
      "         ('could', 'chuck'): 1,\n",
      "         ('chuck', 'wood'): 1})\n"
     ]
    }
   ],
   "source": [
    "pprint(bigram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('how', 'much', 'wood'): 1,\n",
      "         ('much', 'wood', 'does'): 1,\n",
      "         ('wood', 'does', 'a'): 1,\n",
      "         ('does', 'a', 'woodchuck'): 1,\n",
      "         ('a', 'woodchuck', 'chuck'): 1,\n",
      "         ('woodchuck', 'chuck', 'if'): 1,\n",
      "         ('chuck', 'if', 'a'): 1,\n",
      "         ('if', 'a', 'woodchuck'): 1,\n",
      "         ('a', 'woodchuck', 'could'): 1,\n",
      "         ('woodchuck', 'could', 'chuck'): 1,\n",
      "         ('could', 'chuck', 'wood'): 1})\n"
     ]
    }
   ],
   "source": [
    "pprint(trigram_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to have in mind before we move forward: using bigrams and trigrams\n",
    "makes the feature space way larger because of the combinatorial explosion, meaning:\n",
    "\n",
    "\n",
    "- The vocabulary size is $|V|=N$\n",
    "- The size of feature space of the Bag Of Words model is $N$\n",
    "- The size of feature space of the Bigram model is at most $N^2$\n",
    "- The size of feature space of the Trigram model is at most $N^3$\n",
    "\n",
    "\n",
    "\n",
    "## Scikit-Learn Feature Extraction\n",
    "\n",
    "Now that we’ve covered how a text gets transformed to features, we can move on to\n",
    "how this is actually done in practice. Scikit-Learn has special vectorizers for dealing\n",
    "with text that come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t2\n",
      "  (0, 7)\t2\n"
     ]
    }
   ],
   "source": [
    "#Scikit-Learn CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = \"\"\"\n",
    "How much wood does a woodchuck chuck\n",
    "if a woodchuck could chuck wood\n",
    "\"\"\"\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "# \"train\" the vectorizer, aka compute the vocabulary\n",
    "vectorizer.fit([text])\n",
    "# transform text to features\n",
    "print(vectorizer.transform([text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results, you will notice that the right column represents the counts of\n",
    "the words which have exactly the same values as the ones we computed earlier with\n",
    "the Counter function, while the left column represents the indices `(sample_index,\n",
    "word_index_in_vocabulary)`\n",
    "\n",
    "Scikit-Learn works mainly with matrices and almost all components require a\n",
    "matrix as input. The vectorizer transforms a list of texts (notice how both fit\n",
    "and transform get a list of texts as input) into a matrix of size (sample_count, vocabulary_\n",
    "size). The purpose of the fit method is to compute the vocabulary (thus the\n",
    "vocabulary size) by computing how many different words we have. Let’s find out\n",
    "what happens when we use words outside the vocabulary:\n",
    "\n",
    "**Scikit-Learn CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>  (2, 8)\n"
     ]
    }
   ],
   "source": [
    "result = vectorizer.transform([\"Unseen words\", \"BLT sandwich\"])\n",
    "print(type(result), result, result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, we get an empty matrix with all values set to 0. That means\n",
    "none of the known features (words) have been detected and that’s why our print\n",
    "statement doesn’t output anything\n",
    "\n",
    "\n",
    "## Text Classification with Naive Bayes\n",
    "\n",
    "In my opinion, understanding how vectorizers work on text is probably the hardest\n",
    "part of text classification. By now we covered all the essentials and that is well\n",
    "understood, so we’re now ready to read the data and train our classifier.\n",
    "\n",
    "**Scikit-Learn MultinomialNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Naive Bayes classifier for multinomial models\n",
      "\n",
      "    The multinomial Naive Bayes classifier is suitable for classification with\n",
      "    discrete features (e.g., word counts for text classification). The\n",
      "    multinomial distribution normally requires integer feature counts. However,\n",
      "    in practice, fractional counts such as tf-idf may also work.\n",
      "\n",
      "    Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's print a snippet of the documentation for Naive Bayes classifier\n",
    "print(MultinomialNB.__doc__[:415])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./text_analysis_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8dabf77c5828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Remember this is where we saved all the data we crawled previously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./text_analysis_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./text_analysis_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Remember this is where we saved all the data we crawled previously\n",
    "data = pd.read_csv('./text_analysis_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-9-0aada913c67a>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-0aada913c67a>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    with open('./clean_data/{0}'.format(row['file_name']), 'r') as text_file:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Where we keep the actual texts\n",
    "text_samples, labels = []\n",
    "for idx, row in data.iterrows():\n",
    "with open('./clean_data/{0}'.format(row['file_name']), 'r') as text_file:\n",
    "text = text_file.read()\n",
    "text_samples.append(text)\n",
    "labels.append(row['category'])\n",
    "# Split the data for training and for testing and shuffle it\n",
    "# keep 20% for testing, and use the rest for training\n",
    "# shuffling is important because the classes are not random in our dataset\n",
    "labels = data['category'].as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "text_samples, labels, test_size=0.2, shuffle=True)\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "# Compute the vocabulary using only the training data\n",
    "vectorizer.fit(X_train)\n",
    "# Transform the text list to a matrix form\n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "# Vectorize the test data\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "# Check our classifier performance\n",
    "score = classifier.score(X_test_vectorized, y_test)\n",
    "print(\"Accuracy=\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
