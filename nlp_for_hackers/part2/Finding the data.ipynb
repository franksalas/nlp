{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the data\n",
    "\n",
    "Our overall goal in this chapter is to build a service that knows how to classify\n",
    "textual data, the kind we encounter every day. We want our service to tell us which\n",
    "general category a blog post or a news article belongs to. Classifying data like this\n",
    "can be useful in many ways: building the readers’ profile and serve relevant ads,\n",
    "recommend products or personalize content served.\n",
    "\n",
    "\n",
    "### Existing corpora\n",
    "There is a limited number of existing corpora that we could use to achieve our goal.\n",
    "\n",
    "**Check out available corpora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 225 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's try out Reuters corpus\n",
    "from nltk.corpus import reuters\n",
    "# Let's see what are the Reuters categories\n",
    "print(reuters.categories())\n",
    "# ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', ...\n",
    "# Let's check out the 20 newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news20 = fetch_20newsgroups(subset='train')\n",
    "print(list(news20.target_names))\n",
    "# ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, the categories we got aren’t that helpful for our task. They’re\n",
    "narrow in scope and don’t cover the entire news spectrum as we would like.\n",
    "This corpora is mostly used in benchmarking tasks rather than in real-world\n",
    "applications. We can do better by gathering our own data, but bare in mind that\n",
    "usually building a corpus is a tedious and expensive endeavour. If you start from\n",
    "scratch, you need to manually go through a lot of news articles and blog posts\n",
    "and pick the most relevant category for each of them. Most the tutorials on text\n",
    "classification use bogus data, e.g. for doing sentiment analysis, or use an existing\n",
    "corpora that will produce in a useless model with no practical application. In the\n",
    "following section, we will take another path and explore some other ideas for\n",
    "gathering data\n",
    "\n",
    "## Ideas for Gathering Data\n",
    "\n",
    "The goal here is to build a general tiny corpus. This task is complex and but is\n",
    "only adjacent to our main goal of building a news classifier, so we will use some\n",
    "shortcuts. For practical reasons, we could look at these web resources:\n",
    "\n",
    "- Project Gutenberg Categories6 – this can prove useful depending on the domain\n",
    "you are trying to cover. You will need to write a script to download the books,\n",
    "transform them into plain text and use them to train your classifier.\n",
    "\n",
    "- Reddit – This is a good source of already categorized data. Pick a list of\n",
    "subreddits, assign them to a category, crawl the subreddits and extract the\n",
    "links. Consider each of all the articles extracted from the subreddit as belonging\n",
    "to the same category Here’s a list of all subreddits7\n",
    "\n",
    "- Use the Bing Search API8 to get relevant articles for your categories\n",
    "\n",
    "\n",
    "The general idea is to find places on the Internet where content is placed in predefined\n",
    "buckets or categories. These buckets can then be assigned to a category from\n",
    "your own taxonomy. Obviously, the process is quite error-prone. After gathering\n",
    "the data, I suggest looking at some random samples and assessing the percentage\n",
    "of it that’s correctly categorized. This will give you a rough idea of the quality of\n",
    "your corpus.\n",
    "\n",
    "\n",
    "\n",
    "Another trick I like to use after collecting the data is building a script that goes\n",
    "through the labelled data and asks if the sample is correctly classified. Don’t stress\n",
    "too much on the interface, its only purpose is to do the labelling. A command line\n",
    "interface that accepts y/n input, or even a Tinder-like system will do the trick.\n",
    "If the numbers allow you, you can then go manually through the samples that\n",
    "aren’t correctly classified and fix them. It may seem like a lot of work, but keep\n",
    "in mind that if it’s done right, it can save you a lot of time, especially given that the\n",
    "alternative is to search for articles yourself and manually assign the appropriate\n",
    "label.\n",
    "\n",
    "\n",
    "### Getting the Data\n",
    "Getting back to our task at hand, we’ll use a different web resource for building\n",
    "our corpus: the web bookmarking service Pocket9. This service offers an explore\n",
    "feature10 that requires us to input clear, unambiguous queries in order to get well-\n",
    "classified articles. Here’s why this is a really good idea:\n",
    "- data is socially curated and highly qualitative: the articles suggested by Pocket\n",
    "are bookmarked by a big number of users\n",
    "- data is current: suggestions are frequently added to the service\n",
    "- data is easy to gather: the explore feature can be easily crawled and, at this\n",
    "moment, it doesn’t seem to block crawlers\n",
    "\n",
    "If you want to skip the corpus creation step, I already prepared it in advance and\n",
    "you can download from here: Text Classification Data11\n",
    "If you want to get your hands dirty and do it anyway, here’s how we go about it.\n",
    "First, let’s figure out which should be the categories and then proceed to collecting\n",
    "the data.\n",
    "\n",
    "**Category Structure and Keywords**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# If you have specific needs for your corpus, remember to adjust these categories and keywords accordingly.\n",
    "CATEGORIES = {\n",
    "'business': [\n",
    "\"Business\", \"Marketing\", \"Management\"\n",
    "],\n",
    "'family': [\n",
    "\"Family\", \"Children\", \"Parenting\"\n",
    "],\n",
    "'politics': [\n",
    "\"Politics\", \"Presidential Elections\",\n",
    "\"Politicians\", \"Government\", \"Congress\"\n",
    "],\n",
    "'sport': [\n",
    "\"Baseball\", \"Basketball\", \"Running\", \"Sport\",\n",
    "\"Skiing\", \"Gymnastics\", \"Tenis\", \"Football\", \"Soccer\"\n",
    "],\n",
    "'health': [\n",
    "\"Health\", \"Weightloss\", \"Wellness\", \"Well being\",\n",
    "\"Vitamins\", \"Healthy Food\", \"Healthy Diet\"\n",
    "],\n",
    "'economics': [\n",
    "\"Economics\", \"Finance\", \"Accounting\"\n",
    "],\n",
    "'celebrities': [\n",
    "\"Celebrities\", \"Showbiz\"\n",
    "],\n",
    "'medical': [\n",
    "\"Medicine\", \"Doctors\", \"Health System\",\n",
    "\"Surgery\", \"Genetics\", \"Hospital\"\n",
    "],\n",
    "'science & technology': [\n",
    "\"Galaxy\", \"Physics\",\n",
    "\"Technology\", \"Science\"\n",
    "],\n",
    "'information technology': [\n",
    "\"Artificial Intelligence\", \"Search Engine\",\n",
    "\"Software\", \"Hardware\", \"Big Data\",\n",
    "\"Analytics\", \"Programming\"\n",
    "],\n",
    "'education': [\n",
    "\"Education\", \"Students\", \"University\"\n",
    "],\n",
    "'media': [\n",
    "\"Newspaper\", \"Reporters\", \"Social Media\"\n",
    "],\n",
    "'cooking': [\n",
    "\"Cooking\", \"Gastronomy\", \"Cooking Recipes\",\n",
    "\"Paleo Cooking\", \"Vegan Recipes\"\n",
    "],\n",
    "'religion': [\n",
    "\"Religion\", \"Church\", \"Spirituality\"\n",
    "],\n",
    "'legal': [\n",
    "\"Legal\", \"Lawyer\", \"Constitution\"\n",
    "],\n",
    "'history': [\n",
    "\"Archeology\", \"History\", \"Middle Ages\"\n",
    "],\n",
    "'nature & ecology': [\n",
    "\"Nature\", \"Ecology\",\n",
    "\"Endangered Species\", \"Permaculture\"\n",
    "],\n",
    "'travel': [\n",
    "\"Travel\", \"Tourism\", \"Globetrotter\"\n",
    "],\n",
    "'meteorology': [\n",
    "\"Tornado\", \"Meteorology\", \"Weather Prediction\"\n",
    "],\n",
    "'automobiles': [\n",
    "\"Automobiles\", \"Motorcycles\", \"Formula 1\", \"Driving\"\n",
    "],\n",
    "'art & traditions': [\n",
    "\"Art\", \"Artwork\", \"Traditions\",\n",
    "\"Artisan\", \"Pottery\", \"Painting\", \"Artist\"\n",
    "],\n",
    "'beauty & fashion': [\n",
    "\"Beauty\", \"Fashion\", \"Cosmetics\", \"Makeup\"\n",
    "],\n",
    "'relationships': [\n",
    "\"Relationships\", \"Relationship Advice\",\n",
    "\"Marriage\", \"Wedding\"\n",
    "],\n",
    "'astrology': [\n",
    "\"Astrology\", \"Zodiac\", \"Zodiac Signs\", \"Horoscope\"\n",
    "],\n",
    "'diy': [\n",
    "'Gardening', 'Construction', 'Decorating',\n",
    "'Do it Yourself', 'Furniture'\n",
    "]\n",
    "}    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': ['Business', 'Marketing', 'Management'],\n",
       " 'family': ['Family', 'Children', 'Parenting'],\n",
       " 'politics': ['Politics',\n",
       "  'Presidential Elections',\n",
       "  'Politicians',\n",
       "  'Government',\n",
       "  'Congress'],\n",
       " 'sport': ['Baseball',\n",
       "  'Basketball',\n",
       "  'Running',\n",
       "  'Sport',\n",
       "  'Skiing',\n",
       "  'Gymnastics',\n",
       "  'Tenis',\n",
       "  'Football',\n",
       "  'Soccer'],\n",
       " 'health': ['Health',\n",
       "  'Weightloss',\n",
       "  'Wellness',\n",
       "  'Well being',\n",
       "  'Vitamins',\n",
       "  'Healthy Food',\n",
       "  'Healthy Diet'],\n",
       " 'economics': ['Economics', 'Finance', 'Accounting'],\n",
       " 'celebrities': ['Celebrities', 'Showbiz'],\n",
       " 'medical': ['Medicine',\n",
       "  'Doctors',\n",
       "  'Health System',\n",
       "  'Surgery',\n",
       "  'Genetics',\n",
       "  'Hospital'],\n",
       " 'science & technology': ['Galaxy', 'Physics', 'Technology', 'Science'],\n",
       " 'information technology': ['Artificial Intelligence',\n",
       "  'Search Engine',\n",
       "  'Software',\n",
       "  'Hardware',\n",
       "  'Big Data',\n",
       "  'Analytics',\n",
       "  'Programming'],\n",
       " 'education': ['Education', 'Students', 'University'],\n",
       " 'media': ['Newspaper', 'Reporters', 'Social Media'],\n",
       " 'cooking': ['Cooking',\n",
       "  'Gastronomy',\n",
       "  'Cooking Recipes',\n",
       "  'Paleo Cooking',\n",
       "  'Vegan Recipes'],\n",
       " 'religion': ['Religion', 'Church', 'Spirituality'],\n",
       " 'legal': ['Legal', 'Lawyer', 'Constitution'],\n",
       " 'history': ['Archeology', 'History', 'Middle Ages'],\n",
       " 'nature & ecology': ['Nature',\n",
       "  'Ecology',\n",
       "  'Endangered Species',\n",
       "  'Permaculture'],\n",
       " 'travel': ['Travel', 'Tourism', 'Globetrotter'],\n",
       " 'meteorology': ['Tornado', 'Meteorology', 'Weather Prediction'],\n",
       " 'automobiles': ['Automobiles', 'Motorcycles', 'Formula 1', 'Driving'],\n",
       " 'art & traditions': ['Art',\n",
       "  'Artwork',\n",
       "  'Traditions',\n",
       "  'Artisan',\n",
       "  'Pottery',\n",
       "  'Painting',\n",
       "  'Artist'],\n",
       " 'beauty & fashion': ['Beauty', 'Fashion', 'Cosmetics', 'Makeup'],\n",
       " 'relationships': ['Relationships',\n",
       "  'Relationship Advice',\n",
       "  'Marriage',\n",
       "  'Wedding'],\n",
       " 'astrology': ['Astrology', 'Zodiac', 'Zodiac Signs', 'Horoscope'],\n",
       " 'diy': ['Gardening',\n",
       "  'Construction',\n",
       "  'Decorating',\n",
       "  'Do it Yourself',\n",
       "  'Furniture']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, here’s what we’ll be doing next:\n",
    "- querying the service and scraping the article URLs from the page using beautifulsoup412.\n",
    "- iterating through the links and fetching the content of the articles using newspaper3k13\n",
    "a library that helps us extract only the main content of a webpage.\n",
    "- save everything, including the category, in a dataframe and dumping it in a CSV\n",
    "file.\n",
    "\n",
    "**Use Pocket Explore to Build a Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/b6/1fcd64fe7a82b8b207a172ed30a6ee58898b245d281d6d53ed782cee1b13/newspaper3k-0.2.6.tar.gz (197kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 2.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from newspaper3k) (4.6.1)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from newspaper3k) (5.2.0)\n",
      "Collecting PyYAML>=3.11 (from newspaper3k)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Collecting lxml>=3.6.0 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/59/1db3c9c27049e4f832691c6d642df1f5b64763f73942172c44fee22de397/lxml-4.2.4-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.8MB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from newspaper3k) (3.3)\n",
      "Requirement already satisfied: requests>=2.10.0 in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from newspaper3k) (2.19.1)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/0c/1b7332684dfc2e6311d59cd00859a5318a7e0ba50334ad217ceb9555e213/tldextract-2.2.0-py2.py3-none-any.whl (52kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 4.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.4MB 1.4MB/s eta 0:00:01    56% |██████████████████▏             | 4.2MB 18.6MB/s eta 0:00:01    65% |█████████████████████           | 4.9MB 19.1MB/s eta 0:00:01    75% |████████████████████████        | 5.6MB 15.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from newspaper3k) (2.7.3)\n",
      "Requirement already satisfied: six in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from nltk>=3.2.1->newspaper3k) (1.11.0)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from requests>=2.10.0->newspaper3k) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from requests>=2.10.0->newspaper3k) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from requests>=2.10.0->newspaper3k) (2018.8.13)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages (from tldextract>=2.0.1->newspaper3k) (39.2.0)\n",
      "Building wheels for collected packages: newspaper3k, feedparser, feedfinder2, jieba3k\n",
      "  Running setup.py bdist_wheel for newspaper3k ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/frank/.cache/pip/wheels/8c/c4/4a/e445c83be323b5217941405848bab6f8a3d7ccbdd0d05a4d6f\n",
      "  Running setup.py bdist_wheel for feedparser ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/frank/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "  Running setup.py bdist_wheel for feedfinder2 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/frank/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "  Running setup.py bdist_wheel for jieba3k ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/frank/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "Successfully built newspaper3k feedparser feedfinder2 jieba3k\n",
      "\u001b[31mtwisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\u001b[0m\n",
      "\u001b[31mthinc 6.10.2 requires pathlib<2.0.0,>=1.0.0, which is not installed.\u001b[0m\n",
      "\u001b[31mspacy 2.0.11 requires pathlib, which is not installed.\u001b[0m\n",
      "\u001b[31mmkl-random 1.0.1 requires cython, which is not installed.\u001b[0m\n",
      "\u001b[31mspacy 2.0.11 has requirement regex==2017.4.5, but you'll have regex 2017.11.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: PyYAML, cssselect, lxml, feedparser, requests-file, tldextract, feedfinder2, jieba3k, newspaper3k\n",
      "Successfully installed PyYAML-3.13 cssselect-1.0.3 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 lxml-4.2.4 newspaper3k-0.2.6 requests-file-1.4.3 tldextract-2.2.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> make sure you have the directory\n",
    "`data/files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import atexit\n",
    "import urllib\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep, time\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article, ArticleException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "POCKET_BASE_URL = 'https://getpocket.com/explore/%s'\n",
    "df = pd.DataFrame(columns=['title', 'excerpt', 'url', 'file_name', \"keyword\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@atexit.register\n",
    "def save_dataframe():\n",
    "    \"\"\" Before exiting, make sure we save the dataframe to a CSV file \"\"\"\n",
    "    dataframe_name = \"dataframe_{0}.csv\".format(time())\n",
    "    df.to_csv(dataframe_name, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the categories to make sure we are not exhaustively crawling only the first categories\n",
    "categories = list(CATEGORIES.items())\n",
    "random.shuffle(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring Category=\"art & traditions\"\n",
      "Indexing article: \"The Great Chinese Art Heist\" from \"https://www.gq.com/story/the-great-chinese-art-heist\"\n",
      "Indexing article: \"Sentences to ponder\" from \"https://marginalrevolution.com/marginalrevolution/2018/08/sentences-to-ponder-101.html\"\n",
      "Indexing article: \"Is This the Most Powerful Sculpture at the Met?\" from \"https://www.nytimes.com/interactive/2018/08/20/arts/met-buddha-sculpture.html\"\n",
      "Indexing article: \"Chinese Artist Ai Weiwei Uses Ethereum to Make Art About 'Value'\" from \"https://www.coindesk.com/chinese-artist-ai-weiwei-uses-ethereum-to-make-art-about-value/\"\n",
      "Indexing article: \"Mirrored Installations by Sarah Meyohas Create Infinite Tunnels Strewn With Dangling Flowers\" from \"https://www.thisiscolossal.com/2018/08/mirrored-installations-by-sarah-meyohas/\"\n",
      "Indexing article: \"31 Art Exhibitions to View in N.Y.C. This Weekend\" from \"https://www.nytimes.com/2018/08/16/arts/design/art-and-museums-in-nyc-this-week.html\"\n",
      "Indexing article: \"The Lost World of Weegee\" from \"https://www.commentarymagazine.com/articles/lost-world-weegee/\"\n",
      "Indexing article: \"How an Art Conservator Completely Restores a Damaged Painting: A Short, Meditative Documentary\" from \"http://www.openculture.com/2018/08/how-an-art-conservator-completely-restores-a-damaged-painting.html\"\n",
      "Indexing article: \"Art for All: Celebrate Diversity in Design—Volume 12\" from \"https://design.tutsplus.com/articles/art-for-all-celebrate-diversity-in-design-volume-12--cms-31550\"\n",
      "Indexing article: \"Children Of Morta is ridiculously pretty, and looking a rather splendid hack-me-do\" from \"https://www.rockpapershotgun.com/2018/08/20/children-of-morta-is-ridiculously-pretty-and-looking-a-rather-splendid-hack-me-do/\"\n",
      "Indexing article: \"What if There’s No Next Big Thing?\" from \"http://www.e-flux.com/journal/what-if-theres-no-next-big-thing/\"\n",
      "Indexing article: \"The Popular Connoisseur\" from \"http://www.nybooks.com/articles/2018/04/05/popular-connoisseur-kenneth-clark-civilisation/\"\n",
      "Indexing article: \"The Struggling Artist at 86\" from \"https://www.nytimes.com/2018/01/05/nyregion/the-struggling-artist-at-86.html\"\n"
     ]
    }
   ],
   "source": [
    "for category_name, keywords in categories:\n",
    "    print(\"Exploring Category=\\\"{0}\\\"\".format(category_name))\n",
    "\n",
    "    for kw in keywords:\n",
    "        # Get trending content from Pocket's explore endpoint\n",
    "        result = requests.get(POCKET_BASE_URL % urllib.parse.quote_plus(kw))\n",
    "        \n",
    "        # Extract the media items\n",
    "        soup = BeautifulSoup(result.content, \"html5lib\")\n",
    "        media_items = soup.find_all(attrs={'class': 'media_item'})\n",
    "        for item_html in media_items:\n",
    "            title_html = item_html.find_all(attrs={'class': 'title'})[0]\n",
    "            title = title_html.text\n",
    "            \n",
    "            \n",
    "            url = title_html.a['data-saveurl']\n",
    "            \n",
    "            print(\"Indexing article: \\\"{0}\\\" from \\\"{1}\\\"\".format(title, url))\n",
    "            excerpt = item_html.find_all(attrs={'class': 'excerpt'})[0].text\n",
    "            \n",
    "            try:\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                content = article.text\n",
    "            except ArticleException as e:\n",
    "                print(\"Encoutered exception when parsing \\\"{0}\\\": \\\"{1}\\\"\".format(url, str(e)))\n",
    "                continue\n",
    "\n",
    "            if not content:\n",
    "                print(\"Couldn't extract content from \\\"{0}\\\"\".format(url))\n",
    "                continue\n",
    "\n",
    "            # Save the text file\n",
    "                \n",
    "            file_name = \"{0}.txt\".format(str(uuid.uuid4()))\n",
    "            with open('./data/files/{0}'.format(file_name), 'w+') as text_file:\n",
    "                text_file.write(content)\n",
    "            \n",
    "            # Append the row in our dataframe\n",
    "            df.loc[len(df)] = [title, excerpt, url, file_name, kw, category_name]\n",
    "            # Need to sleep in order to not get blocked\n",
    "            sleep(random.randint(5, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mGetting Started with Scikit-Learn.ipynb\u001b[0m*\r\n",
      "\u001b[01;32mIntroduction to Machine Learning.ipynb\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
