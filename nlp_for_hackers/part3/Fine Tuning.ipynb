{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "In this chapter we will go over several strategies for boosting the accuracy of a\n",
    "classifier.\n",
    "\n",
    "## Try a different classifier\n",
    "\n",
    "One of the classifiers that is always worth trying out is the LogisticRegression one\n",
    "from nltk. It is very versatile and especially good with text. The main advantage\n",
    "of this classifier is that it doesn’t need any parameter adjustments, just like the\n",
    "Naive Bayes we’ve been experimenting with. Only change you need to make to the\n",
    "previous script to try this out is:\n",
    "\n",
    "**Try LogisticRegression Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./twitter_sentiment_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data['tweet'].values.astype(str)\n",
    "sentiments = data['sentiment'].values.astype(str)\n",
    "# Split the data for training and for testing and shuffle it\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, sentiments,\n",
    "test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.826781051846326\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "# Compute the vocabulary only on the training data\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# Transform the text list to a matrix form\n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Vectorize the test data\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Check our classifier performance\n",
    "score = classifier.score(X_test_vectorized, y_test)\n",
    "print(\"Accuracy=\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by doing this change, we got a boost up to 0.82 in accuracy. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ngrams Instead of Words\n",
    "\n",
    "The `Scikit-Learn` vectorizer API allows us to use ngrams rather than just words.\n",
    "Remember what we’ve covered in the previous chapters about ngrams? It’s exactly\n",
    "the same procedure. Instead of using only single-word features, we use consecutive,\n",
    "multi-word features as well. Changes to the previous script to make this happen are\n",
    "minimal\n",
    "**Using Ngram Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "tweets = data['tweet'].values.astype(str)\n",
    "sentiments = data['sentiment'].values.astype(str)\n",
    "# Split the data for training and for testing and shuffle it\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, sentiments,\n",
    "test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.9376352107422603\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, tokenizer=tweet_tokenizer.tokenize, ngram_range=(1, 3))\n",
    "\n",
    "\n",
    "# Compute the vocabulary only on the training data\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# Transform the text list to a matrix form\n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Vectorize the test data\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Check our classifier performance\n",
    "score = classifier.score(X_test_vectorized, y_test)\n",
    "print(\"Accuracy=\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy day, now we’re up to 0.93 in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pipeline\n",
    "\n",
    "Using a pipeline has a bunch of benefits:\n",
    "\n",
    "- the ansamble of components behaves as a single classifier\n",
    "- the code is clean and encapsulated\n",
    "- it is easy to iterate on improving the model (more about that in the following\n",
    "section)\n",
    "\n",
    "**Using a Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize.casual import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(strip_handles=True)\n",
    "data = pd.read_csv('./twitter_sentiment_analysis.csv')\n",
    "\n",
    "tweets = data['tweet'].values.astype(str)\n",
    "sentiments = data['sentiment'].values.astype(str)\n",
    "# Split the data for training and for testing and shuffle it\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, sentiments,\n",
    "test_size=0.2, shuffle=True)\n",
    "\n",
    "# Put everything in a Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(lowercase=True, tokenizer=tweet_tokenizer.tokenize,ngram_range=(1, 3))),('classifier', LogisticRegression())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "# Check our classifier performance\n",
    "\n",
    "score = pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy=\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "\n",
    "This strategy might seem a bit harder to grasp, but bare with me and we’ll get to the\n",
    "bottom of it.\n",
    "\n",
    "I stated earlier in the book that we need to keep the test data separate from the\n",
    "train data, in order to not influence classifier’s output. Did you wonder why that is?\n",
    "Well, if we test the system with the same data we trained on, obviously we would\n",
    "get awesome results, but biased. In order to get valid results, we need to test the\n",
    "system with data it hasn’t seen yet.\n",
    "\n",
    "If we continuously tweak the parameters to improve the results on the test set, we\n",
    "indirectly overfit the system on the test dataset. That would be an undesired result\n",
    "because it makes the system worse at generalizing. That means that if we will test\n",
    "on unseed data, outside of the test set, it will underperform. One way to fix this\n",
    "problem is to keep some more data aside and never test on this data while we tune\n",
    "the parameters. This type of data is called the Validation Set. After we’re satisfied\n",
    "with the results on the test set, then and only then we use the Validation Set to check\n",
    "how our system is doing on unseen data.\n",
    "\n",
    "This approach has a huge drawback: Data is usually scarce, and we will be putting\n",
    "even more data aside that’s not going to be used for training.\n",
    "\n",
    "An approach for getting around this drawback would be doing Cross Validation.\n",
    "This implies splitting the dataset into N folds. The system will be trained N times on\n",
    "all the data, each time excluding a different fold, out of the N total ones.\n",
    "At the end, the scores of all trains are averaged. This way we don’t waste much data.\n",
    "Here’s an example of Cross Validation with N = 5 folds we do that:\n",
    "**Cross Validation Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.tokenize.casual import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(strip_handles=True)\n",
    "data = pd.read_csv('./twitter_sentiment_analysis.csv')\n",
    "tweets = data['tweet'].values.astype(str)\n",
    "sentiments = data['sentiment'].values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything in a Pipeline\n",
    "pipeline = Pipeline([\n",
    "('vectorizer', CountVectorizer(\n",
    "lowercase=True,\n",
    "tokenizer=tweet_tokenizer.tokenize,\n",
    "ngram_range=(1, 3))),\n",
    "('classifier', LogisticRegression())\n",
    "])\n",
    "tweets, sentiments = shuffle(tweets, sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 3 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"MeanAccuracy=\", cross_val_score(pipeline, tweets, sentiments, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Cross Validation strategy, we’re still around 0.82 in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "As we’ve seen so far, there are quite a few parameters we can tune to improve\n",
    "accuracy, and we have not explored that many yet. Moreover, there’s no way to\n",
    "know for sure what will be the effects of tuning a parameter in a certain way. There’s\n",
    "no exact algorithm for tuning a model. Mastering this implies curiosity and lots of\n",
    "practice.\n",
    "\n",
    "However, here’s a simple way of optimizing parameter combinations called Grid\n",
    "Search. This technique implies using Cross Validation for every possible parameter\n",
    "combination. That’s a lot of work, so it will take a while\n",
    "**Tuning Parameters with GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize.casual import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(strip_handles=True)\n",
    "data = pd.read_csv('./twitter_sentiment_analysis.csv')\n",
    "tweets = data['tweet'].values.astype(str)\n",
    "sentiments = data['sentiment'].values.astype(str)\n",
    "# Shuffle the data\n",
    "tweets, sentiments = shuffle(tweets, sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything in a Pipeline\n",
    "pipeline = Pipeline([\n",
    "('vectorizer', CountVectorizer(\n",
    "lowercase=True,\n",
    "tokenizer=tweet_tokenizer.tokenize,\n",
    "ngram_range=(1, 3))),\n",
    "('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "classifier = GridSearchCV(pipeline, {\n",
    "# try out different ngram ranges\n",
    "'vectorizer__ngram_range': ((1, 2), (2, 3), (1, 3)),\n",
    "# check if setting all non zero counts to 1 makes a difference\n",
    "'vectorizer__binary': (True, False),\n",
    "}, n_jobs=-1, verbose=True, error_score=0.0, cv=5)\n",
    "# Compute the vocabulary and train the classifier\n",
    "classifier.fit(tweets, sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Accuracy: \", classifier.best_score_)\n",
    "print(\"Best Parameters: \", classifier.best_params_)\n",
    "# Best Accuracy: 0.81920859947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
