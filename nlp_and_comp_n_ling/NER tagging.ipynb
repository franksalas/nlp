{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER-Tagging and Its Applications\n",
    "\n",
    "## What is NER-tagging?\n",
    "\n",
    "- Named Entity Recogitnio\n",
    "- name entity is a real-wordl object with a proper name\n",
    "    - ex: France, Donald Duck, and Twitter.\n",
    "    - France is a country identified as **GPE** Geopolitical Entity\n",
    "    - Donald Duck is a **PER**(  person)\n",
    "    - Twitter is a company  **ORG**\n",
    "    \n",
    "![](https://i.imgur.com/fF1EFTJ.png)\n",
    "\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/PmqvReT.png)\n",
    "\n",
    "\n",
    "> Rome is the capital of Italy\n",
    "\n",
    "- NER-tagger regognize Rome as a plac(GPE as well as Italy\n",
    "- it knows that Rome is a city in italy and not a person by **Named Entity Disambiguatoin(NED)**\n",
    "\n",
    "- Perform POS tagging before NER tagging\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have our features ready there are many ML models we can use to train our model\n",
    "\n",
    "- **CRF** Conditional Random fields\n",
    "    - popular choise for NER- taggin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ner tagging in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "B-{CHUNK_TYPE}- for the word in te begining chinck\n",
    "\n",
    "![](https://i.imgur.com/OqfZ0uW.png)\n",
    "Fig 6.3 spaCy's own BILOU system for its NER tags\n",
    "\n",
    "---\n",
    "Even though we will largely use spaCy, let's briefly discuss NLTK: NLTK uses these chunks as part of a tree-like system to do its tagging, though it also has a tagger which follows an IOB system. Here are some code snippets explaining how to use both, and how to convert between them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags \n",
    "from nltk import pos_tag \n",
    "from nltk import word_tokenize\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our imports, where these models are trained on the CoNLL (from the CoNLL conference) corpus in NLTK. Since we already did our tokenizing, POS-tagging and chunking, all we need to do for the tree-based tagging is to use the conlltags2tree method to see our tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Clement', 'NN', 'B-GPE'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('Mathieu', 'NNP', 'B-PERSON'),\n",
       " ('are', 'VBP', 'O'),\n",
       " ('working', 'VBG', 'O'),\n",
       " ('at', 'IN', 'O'),\n",
       " ('Apple', 'NNP', 'B-ORGANIZATION'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Clement and Mathieu are working at Apple.\" \n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(sentence))) \n",
    "\n",
    "iob_tagged = tree2conlltags(ne_tree)\n",
    "iob_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here how we first tokenized our sentence, then POS-tagged it, and chunked it before passing it to the tree-based tagger. Our output is each word tagged appropriately with both the part of speech and named entity class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Clement/NN)\n",
      "  and/CC\n",
      "  (PERSON Mathieu/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Apple/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ne_tree = conlltags2tree(iob_tagged)\n",
    "\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER-tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_0 = nlp(u'Donald Trump visited at the government headquarters in France today.') \n",
    " \n",
    "sent_1 = nlp(u'Emmanuel Jean-Michel Frédéric Macron is a French politician serving as President of France and ex officio Co-Prince of Andorra since 14 May 2017.') \n",
    " \n",
    "sent_2 = nlp(u\"He studied philosophy at Paris Nanterre University, completed a Master's of Public Affairs at Sciences Po, and graduated from the École nationale d'administration (ÉNA) in 2004.\") \n",
    " \n",
    "sent_3 = nlp(u'He worked at the Inspectorate General of Finances, and later became an investment banker at Rothschild & Cie Banque.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Donald', 'PERSON')\n",
      "('Trump', 'PERSON')\n",
      "('visited', '')\n",
      "('at', '')\n",
      "('the', '')\n",
      "('government', '')\n",
      "('headquarters', '')\n",
      "('in', '')\n",
      "('France', 'GPE')\n",
      "('today', 'DATE')\n",
      "('.', '')\n"
     ]
    }
   ],
   "source": [
    "# sent_0\n",
    "for token in sent_0:\n",
    "    print((token.text, token.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Emmanuel', 'PERSON')\n",
      "('Jean', 'PERSON')\n",
      "('-', 'PERSON')\n",
      "('Michel', 'PERSON')\n",
      "('Frédéric', 'PERSON')\n",
      "('Macron', 'PERSON')\n",
      "('is', '')\n",
      "('a', '')\n",
      "('French', 'NORP')\n",
      "('politician', '')\n",
      "('serving', '')\n",
      "('as', '')\n",
      "('President', '')\n",
      "('of', '')\n",
      "('France', 'GPE')\n",
      "('and', '')\n",
      "('ex', '')\n",
      "('officio', '')\n",
      "('Co', 'LOC')\n",
      "('-', 'LOC')\n",
      "('Prince', 'LOC')\n",
      "('of', 'LOC')\n",
      "('Andorra', 'LOC')\n",
      "('since', '')\n",
      "('14', 'DATE')\n",
      "('May', 'DATE')\n",
      "('2017', 'DATE')\n",
      "('.', '')\n"
     ]
    }
   ],
   "source": [
    "# sent_1\n",
    "for token in sent_1:\n",
    "    print((token.text, token.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('He', '')\n",
      "('studied', '')\n",
      "('philosophy', '')\n",
      "('at', '')\n",
      "('Paris', 'ORG')\n",
      "('Nanterre', 'ORG')\n",
      "('University', 'ORG')\n",
      "(',', '')\n",
      "('completed', '')\n",
      "('a', '')\n",
      "('Master', 'WORK_OF_ART')\n",
      "(\"'s\", 'WORK_OF_ART')\n",
      "('of', 'WORK_OF_ART')\n",
      "('Public', 'WORK_OF_ART')\n",
      "('Affairs', 'WORK_OF_ART')\n",
      "('at', 'WORK_OF_ART')\n",
      "('Sciences', 'WORK_OF_ART')\n",
      "('Po', 'WORK_OF_ART')\n",
      "(',', '')\n",
      "('and', '')\n",
      "('graduated', '')\n",
      "('from', '')\n",
      "('the', '')\n",
      "('École', '')\n",
      "('nationale', '')\n",
      "(\"d'administration\", 'PRODUCT')\n",
      "('(', 'PRODUCT')\n",
      "('ÉNA', 'ORG')\n",
      "(')', '')\n",
      "('in', '')\n",
      "('2004', 'DATE')\n",
      "('.', '')\n"
     ]
    }
   ],
   "source": [
    "# sent_2\n",
    "for token in sent_2:\n",
    "    print((token.text, token.ent_type_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('He', '')\n",
      "('worked', '')\n",
      "('at', '')\n",
      "('the', 'ORG')\n",
      "('Inspectorate', 'ORG')\n",
      "('General', 'ORG')\n",
      "('of', 'ORG')\n",
      "('Finances', 'ORG')\n",
      "(',', '')\n",
      "('and', '')\n",
      "('later', '')\n",
      "('became', '')\n",
      "('an', '')\n",
      "('investment', '')\n",
      "('banker', '')\n",
      "('at', '')\n",
      "('Rothschild', 'ORG')\n",
      "('&', 'ORG')\n",
      "('Cie', 'ORG')\n",
      "('Banque', 'ORG')\n",
      "('.', '')\n"
     ]
    }
   ],
   "source": [
    "# sent_3\n",
    "for token in sent_3:\n",
    "    print((token.text, token.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the Inspectorate General of Finances', 'ORG')\n",
      "('Rothschild & Cie Banque', 'ORG')\n"
     ]
    }
   ],
   "source": [
    "for ent in sent_3.ents:\n",
    "    print((ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training our own NER-taggers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    ('Who is Shaka Khan?', {\n",
    "        'entities': [(7, 17, 'PERSON')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-m None] [-o None] [-n 100]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/frank/.local/share/jupyter/runtime/kernel-6463ef05-1e4b-4402-b66e-b116a21c1474.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    plac.call(main)\n",
    "\n",
    "    # Expected output:\n",
    "    # Entities [('Shaka Khan', 'PERSON')]\n",
    "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
