{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim â€“ Vectorizing Text and Transformations and n-grams\n",
    "\n",
    "## intro\n",
    "- vector spaces\n",
    "- bag-of-words\n",
    "- TF-IDF (term frequency-inverse doc frequency)\n",
    "- LSI (latent sematinc indexing)\n",
    "- word2vec\n",
    "\n",
    "---\n",
    "Primary features of Gensim are memory-independent nature, mulicore implementation of lenten sematic analysis, latent Dirchelt allocation, random projection, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorsn and why we need them\n",
    "\n",
    "- expected to pass vectos as input to IR algorithms (ex LDA LSI) bc it invoves matrices\n",
    "- these models are called **Vector SpaceModels**\n",
    "\n",
    "- ML algorithms use these vectos tomake predictions.\n",
    "- purpose is to learn from tehprovided data by decreasing the error of their predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "- most straightfoward form of representing s sentence as a vector\n",
    "\n",
    "```\n",
    "S1:\"The dog sat by the mat.\"\n",
    "S2:\"The cat loves the dog.\"\n",
    "```\n",
    "\n",
    "basic processing\n",
    "```\n",
    "S1:\"dog sat mat.\"\n",
    "S2:\"cat love dog.\"\n",
    "\n",
    "```\n",
    "as a python list\n",
    "\n",
    "```python\n",
    "S1:['dog', 'sat', 'mat']\n",
    "S2:['cat', 'love', 'dog']\n",
    "```\n",
    "\n",
    "If we wan to represent this as a vector, we need to first constric our vocabulary. whic woulde be the unique wors found in the sentece\n",
    "\n",
    "```python\n",
    "Vocab = ['dog', 'sat', 'mat', 'love', 'cat']\n",
    "```\n",
    "- this is represented as a vecor with a length of $5$\n",
    "- or our vector has $5$ dimensions\n",
    "\n",
    "The bag-of-words model involves frquencies to construct our vectors.\n",
    "\n",
    "```\n",
    "S1:[1, 1, 1, 0, 0]\n",
    "S2:[1, 0, 0, 1, 1]\n",
    "```\n",
    "- ther is 1 occurence of `dog`, \n",
    "- 0 occurences of `lov3` in  the first sentence\n",
    "- if the first sentence has 2 occurences of the word `dog` it woudl be represented as:\n",
    "```\n",
    "S1: [2, 1, 1, 0, 0]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "*term frequency-inverse document frequency*\n",
    "\n",
    "-  used in search engines to find relevant docs based on a query\n",
    "- tries to encode two diffent kinds of information\n",
    "    - term frequency:**TF** is the number of times a word appears in a document\n",
    "    - inverse document frequency: helps us understand the i,prtance of a word ina document by calculating the logarithmically scaled invese fraction of the doucments tha tcontain the word( obtained by dividien the total num of documents by the  number of docs containing the term) and then takinng the log of that quotent\n",
    "    \n",
    "$$\n",
    "TF(t)= \\frac{\\text{number of times term t appears in a doc}}{\\text{total number of terms in the doc}}\\\\\n",
    "\\\\\n",
    "IDF(t) = log_e\\frac{\\text{total num of docs}}{\\text{num of docs with term } t \\text{ in it}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF** is the product of these two factors\n",
    "\n",
    "- makes rare words more prominent and ignores common words such as *is, of ,that* which may appaer a lot of times, but have little importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector transfromations in Gensim\n",
    "- corpus is a collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "documents = [\"Football club Arsenal defeat local rivals this weekend.\", u\"Weekend football frenzy takes over London.\", u\"Bank open for takeover bids after losing millions.\", u\"London football clubs bid to move to Wembley stadium.\", u\"Arsenal bid 50 million pounds for striker Kane.\", u\"Financial troubles result in loss of millions for bank.\", u\"Western bank files for bankruptcy after financial losses.\", u\"London football club is taken over by oil millionaire from Russia.\", u\"Banking on finances not working for Russia.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'],\n",
       " ['weekend', 'football', 'frenzy', 'take', 'london'],\n",
       " ['bank', 'open', 'takeover', 'bid', 'lose', 'million'],\n",
       " ['london', 'football', 'club', 'bid', 'wembley', 'stadium'],\n",
       " ['arsenal', 'bid', 'pound', 'striker', 'kane'],\n",
       " ['financial', 'trouble', 'result', 'loss', 'million', 'bank'],\n",
       " ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'],\n",
       " ['london', 'football', 'club', 'take', 'oil', 'millionaire', 'russia'],\n",
       " ['bank', 'finance', 'work', 'russia']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "texts = []\n",
    "\n",
    "for document in documents:\n",
    "    text = []\n",
    "    doc = nlp(document)\n",
    "    for w in doc:\n",
    "        # no stop words, no punctuation, no nums, no stems, no seeds..\n",
    "        if not w.is_stop and not w.is_punct and not w.like_num:\n",
    "            text.append(w.lemma_)\n",
    "    texts.append(text)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Football club Arsenal defeat local rivals this weekend.',\n",
       " 'Weekend football frenzy takes over London.',\n",
       " 'Bank open for takeover bids after losing millions.',\n",
       " 'London football clubs bid to move to Wembley stadium.',\n",
       " 'Arsenal bid 50 million pounds for striker Kane.',\n",
       " 'Financial troubles result in loss of millions for bank.',\n",
       " 'Western bank files for bankruptcy after financial losses.',\n",
       " 'London football club is taken over by oil millionaire from Russia.',\n",
       " 'Banking on finances not working for Russia.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'],\n",
       " ['weekend', 'football', 'frenzy', 'take', 'london'],\n",
       " ['bank', 'open', 'takeover', 'bid', 'lose', 'million'],\n",
       " ['london', 'football', 'club', 'bid', 'wembley', 'stadium'],\n",
       " ['arsenal', 'bid', 'pound', 'striker', 'kane'],\n",
       " ['financial', 'trouble', 'result', 'loss', 'million', 'bank'],\n",
       " ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'],\n",
       " ['london', 'football', 'club', 'take', 'oil', 'millionaire', 'russia'],\n",
       " ['bank', 'finance', 'work', 'russia']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a bag-of-words representation for our mini-corpus.\n",
    "- gensim allows us to do it through its `dictionary` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arsenal': 0,\n",
       " 'club': 1,\n",
       " 'defeat': 2,\n",
       " 'football': 3,\n",
       " 'local': 4,\n",
       " 'rival': 5,\n",
       " 'weekend': 6,\n",
       " 'frenzy': 7,\n",
       " 'london': 8,\n",
       " 'take': 9,\n",
       " 'bank': 10,\n",
       " 'bid': 11,\n",
       " 'lose': 12,\n",
       " 'million': 13,\n",
       " 'open': 14,\n",
       " 'takeover': 15,\n",
       " 'stadium': 16,\n",
       " 'wembley': 17,\n",
       " 'kane': 18,\n",
       " 'pound': 19,\n",
       " 'striker': 20,\n",
       " 'financial': 21,\n",
       " 'loss': 22,\n",
       " 'result': 23,\n",
       " 'trouble': 24,\n",
       " 'bankruptcy': 25,\n",
       " 'file': 26,\n",
       " 'western': 27,\n",
       " 'millionaire': 28,\n",
       " 'oil': 29,\n",
       " 'russia': 30,\n",
       " 'finance': 31,\n",
       " 'work': 32}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 32 unique wores in our corpus\n",
    "- all represented in our dict with a value\n",
    "- a words `integer-id` mapping \n",
    "\n",
    "- we will use `doc2bow` method to  convert our docuemtn to bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)],\n",
       " [(1, 1), (3, 1), (8, 1), (11, 1), (16, 1), (17, 1)],\n",
       " [(0, 1), (11, 1), (18, 1), (19, 1), (20, 1)],\n",
       " [(10, 1), (13, 1), (21, 1), (22, 1), (23, 1), (24, 1)],\n",
       " [(10, 1), (21, 1), (22, 1), (25, 1), (26, 1), (27, 1)],\n",
       " [(1, 1), (3, 1), (8, 1), (9, 1), (28, 1), (29, 1), (30, 1)],\n",
       " [(10, 1), (30, 1), (31, 1), (32, 1)]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is a list of lists, where each individual list represents a docuemtn of bag-of-words representation.\n",
    "- `(word_id, word_count)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously mentioned how Gensim is powerful because it uses streaming corpuses. But in this case, the entire list is loaded into the RAM. This is not a bother for us because it is a toy example, but in any real-world cases, this might cause problems. How do we get past this?\n",
    "\n",
    "We can start by storing the corpus, once it is created, to disk. One way to do this is as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('docs/tmp/example.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By storing the corpus to disk and then later loading from disk, we are being far more memory efficient, because at most one vector resides in the RAM at a time. The Gensim tutorial [13] on corpora and vector spaces covers a little more than what we discussed so far and may be useful for some readers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "converting a bag of words representation into TF-IDF is easy with Gensim\n",
    "- choose the model/representtion we want fro teh gensim models dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this means that `tfidf` now represents a **TF-IDF** table *trained* on our corpus.\n",
    "\n",
    "- in the case of **TFIDF**, the *training* consis simply of going thhrough teh supplied corpus one and computing docuemnt frequencies of all its features.\n",
    "\n",
    "So what does a **TF-IDF** repredentation of our corpus look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3292179861221233), (1, 0.24046829370585296), (2, 0.4809365874117059), (3, 0.1774993848325406), (4, 0.4809365874117059), (5, 0.4809365874117059), (6, 0.3292179861221233)]\n",
      "[(3, 0.24212967666975266), (6, 0.4490913847888623), (7, 0.6560530929079719), (8, 0.32802654645398593), (9, 0.4490913847888623)]\n",
      "[(10, 0.18797844084016113), (11, 0.25466485399352906), (12, 0.5093297079870581), (13, 0.3486540744136096), (14, 0.5093297079870581), (15, 0.5093297079870581)]\n",
      "[(1, 0.29431054749542984), (3, 0.21724253258131512), (8, 0.29431054749542984), (11, 0.29431054749542984), (16, 0.5886210949908597), (17, 0.5886210949908597)]\n",
      "[(0, 0.354982288765831), (11, 0.25928712547209604), (18, 0.5185742509441921), (19, 0.5185742509441921), (20, 0.5185742509441921)]\n",
      "[(10, 0.19610384738673725), (13, 0.3637247180792822), (21, 0.3637247180792822), (22, 0.3637247180792822), (23, 0.5313455887718271), (24, 0.5313455887718271)]\n",
      "[(10, 0.18286519950508276), (21, 0.3391702611796705), (22, 0.3391702611796705), (25, 0.4954753228542582), (26, 0.4954753228542582), (27, 0.4954753228542582)]\n",
      "[(1, 0.2645025265769199), (3, 0.1952400253294319), (8, 0.2645025265769199), (9, 0.3621225392416359), (28, 0.5290050531538398), (29, 0.5290050531538398), (30, 0.3621225392416359)]\n",
      "[(10, 0.22867660961662029), (30, 0.4241392327204109), (31, 0.6196018558242014), (32, 0.6196018558242014)]\n"
     ]
    }
   ],
   "source": [
    "for document in tfidf[corpus]:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (`word_id`, `product of the TF and IDF score for this word`)\n",
    "- the higher the score the more important the wored in teh document.\n",
    "- we can use this representation as input for our ML algo as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams and some more preprocessing\n",
    "\n",
    "- $n$-gram is a contigus swquence of $n$ items in the text, in our case, we willbe dealing with words being the *ithem*\n",
    "- but other cases could be letters, sylables or somethimes the cas eo fspeach,\n",
    "- a `bi-gram` is when $n=2$\n",
    "\n",
    "### bigram calcuations\n",
    "- the conditoina probability of a token given by the receding token\n",
    "- by choosing words that appear next to each other\n",
    "- more likely to appear as a pair are called  a collacation.\n",
    "    - ex: New York\n",
    "    - machine learning\n",
    "    - we identify with high probaility that th word York follows the word New, and  is worth considering  \"New York\" as one identity\n",
    "- must get rid of stop words before running a bi-gram model on our corpus** as ther ecould be meaningless bi-grams formed.\n",
    "\n",
    "The Gensim **bi-gram** model is basically an implementation of collocation identification.\n",
    "\n",
    "- Gensim approaches  bigram by simply combinint hetwo high probability tokens with an underscore.\n",
    "- the token new and yoru will now become `new_york` instead\n",
    "- TF-IDF model, bigrams can be cfreate usign another Gensim model **Phrases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "bigram = gensim.models.Phrases(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we now have train bi-gram model fro our corpus.\n",
    "- we can perform our *transformation*  ont eh text the same waw we used TF-IDF\n",
    "- we create our corpus like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "texts = [bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each line will now have all possible bi-grams created\n",
    "\n",
    "- Since by creating new phrases we add words to our dictionary, this step must be done before we create our dictionary. We would have to run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, one popular preprocessing technique involves removing both high frequency and low-frequency words. We can do this in Gensim with the dictionary module. Let's say we would like to get rid of words that occur in less than 20 documents, or in more than 50% of the documents, we would add the following:\n",
    "\n",
    "```python\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We've seen in this chapter why it makes sense to change our representation of text from words to numbers, and why this is the only language a computer understands. There are different ways computers can interpret words, and TF-IDF and bag of words are two such vector representations. Gensim is a Python package that offers us ways to generate such vector representations, which are later used as inputs into various machine learning and information retrieval algorithms.\n",
    "\n",
    "There are further preprocessing techniques such as creating n-grams, collocations and removing low-frequency words, which can help us arrive at better results. The concepts of vectors form a basis in natural language processing and we can now get back to using spaCy's pipelines; indeed, Chapter 5, POS-Tagging and Its Applications, Chapter 6, NER-Tagging and Its Applications, and Chapter 7, Dependency Parsing, all showcase the power of spaCy, and we will start with POS-tagging algorithms using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
