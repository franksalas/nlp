{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagging and Its Applications\n",
    "\n",
    "- part-of-speach tagging\n",
    "- process of tagging word in a texual input with theri appropriate par tof speach\n",
    "\n",
    "examples\n",
    "\n",
    "- Noun - The name of a person, place, thing, or idea\n",
    "- Verb - The action or being\n",
    "- Adjective - This modifies or describes a noun or a pronoun\n",
    "- Adverb - This modifies or describes a verb, adjective, or another adverb\n",
    "- Pronoun - The word to be used in place of a noun\n",
    "- Preposition - The word placed before a noun or pronoun to form a phrase modifying another word in the sentence\n",
    "- Conjunction - This joins words, phrases, or clauses\n",
    "- Interjection - A word used to express emotion\n",
    "\n",
    "\n",
    "Even within the English language, POS-tagging isn't always a straightforward task and words have different POS-tags depending on the context. A simple example is the word refuse, where if it used as a verb it means to decline an offer, and when used as a noun it is used to refer to something you throw away or rubbish.\n",
    "\n",
    "\n",
    "In spaCy, for a more detailed analysis, we also have the .tag_ attribute, which adds more information to the previously given .pos_ attribute. The following table gives the breakup of the categories spaCy has to annotate its words.\n",
    "\n",
    "![](https://i.imgur.com/hXfl5Sa.png)\n",
    "\n",
    "\n",
    "\n",
    "### why\n",
    "\n",
    "-  speach to text conversion and lang translations\n",
    "- dependency parsing\n",
    "    - process of identifyin gdependecis or relationships between wors in a sentence or phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's fairly straightforward API for playing around or sandboxing is what usually tends to make it an attractive choice for beginners. To get the appropriate tags for a sentence, all we have to run is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import particular tagger, use the `train_sents` object ate the tainingsentences you wish to use to treain the `bigram`tagger\n",
    "\n",
    "```python\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "bigram_tagger.tag(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sentences we woudl like to pos-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_0 = nlp(u'Mathieu and I went to the park.')\n",
    "sent_1 = nlp(u'If Clement was asked to take out the garbage, he would refuse.')\n",
    "sent_2 = nlp(u'Baptiste was in charge of the refuse treatment center.')\n",
    "sent_3 = nlp(u'Marie took out her rather suspicious and fishy cat to go fish for fish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mathieu', 'PROPN', 'NNP')\n",
      "('and', 'CCONJ', 'CC')\n",
      "('I', 'PRON', 'PRP')\n",
      "('went', 'VERB', 'VBD')\n",
      "('to', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('park', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "# sent_0\n",
    "\n",
    "for token in sent_0:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results for `sent_0`\n",
    "- matiew is a name and correcly marked as proper noun\n",
    "- went is a verb\n",
    "- park is a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('If', 'ADP', 'IN')\n",
      "('Clement', 'PROPN', 'NNP')\n",
      "('was', 'VERB', 'VBD')\n",
      "('asked', 'VERB', 'VBN')\n",
      "('to', 'PART', 'TO')\n",
      "('take', 'VERB', 'VB')\n",
      "('out', 'PART', 'RP')\n",
      "('the', 'DET', 'DT')\n",
      "('garbage', 'NOUN', 'NN')\n",
      "(',', 'PUNCT', ',')\n",
      "('he', 'PRON', 'PRP')\n",
      "('would', 'VERB', 'MD')\n",
      "('refuse', 'VERB', 'VB')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "# sent_1\n",
    "\n",
    "for token in sent_1:\n",
    "    print((token.text, token.pos_, token.tag_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- refuse is a verb\n",
    "- garbage is a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Baptiste', 'PROPN', 'NNP')\n",
      "('was', 'VERB', 'VBD')\n",
      "('in', 'ADP', 'IN')\n",
      "('charge', 'NOUN', 'NN')\n",
      "('of', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('refuse', 'ADJ', 'JJ')\n",
      "('treatment', 'NOUN', 'NN')\n",
      "('center', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "# sent_2\n",
    "\n",
    "for token in sent_2:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- refuse is incorrecly tagged as a noun\n",
    "- something `Baptiste` in charge of \n",
    "- three words are all nouns called *nounphrase*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marie', 'PROPN', 'NNP')\n",
      "('took', 'VERB', 'VBD')\n",
      "('out', 'PART', 'RP')\n",
      "('her', 'PRON', 'PRP')\n",
      "('rather', 'ADV', 'RB')\n",
      "('suspicious', 'ADJ', 'JJ')\n",
      "('and', 'CCONJ', 'CC')\n",
      "('fishy', 'ADJ', 'JJ')\n",
      "('cat', 'NOUN', 'NN')\n",
      "('to', 'PART', 'TO')\n",
      "('go', 'VERB', 'VB')\n",
      "('fish', 'NOUN', 'NN')\n",
      "('for', 'ADP', 'IN')\n",
      "('fish', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "# sent_3\n",
    "\n",
    "for token in sent_3:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- made to fool our tagger with diffrent variations of the word fish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainign our won POS-taggers\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/RsgNAkz.png)\n",
    "\n",
    "\n",
    "a simple trainign loop would look like this\n",
    "\n",
    "```python\n",
    "TRAIN_DATA = [ \n",
    "     (\"Facebook has been accused for leaking personal data of users.\", {'entities': [(0, 8, 'ORG')]}), \n",
    "     (\"Tinder uses sophisticated algorithms to find the perfect match.\", {'entities': [(0, 6, \"ORG\")]})]\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk('/model')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a POS-tagger isn't any different in theory, and we will be using the example code (train_tagger.py [18]) in the spaCy GitHub page which guides us in how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "\n",
    "TAG_MAP = {\n",
    "    'N': {'pos': 'NOUN'},\n",
    "    'V': {'pos': 'VERB'},\n",
    "    'J': {'pos': 'ADJ'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- setup basic imports\n",
    "- initialize the TAG_MAP dict\n",
    "- we need to define a mapping form the datas part-of-speach tag names to the **universal part-of-speach tag** set\n",
    "-  in this example we only intedn to train nouns, verbs and adjetives so we includ ethese in our tag map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small set, mre data results ina better model\n",
    "# jsut an idea of how the trainig data should look like\n",
    "TRAIN_DATA = [\n",
    "    (\"I like green eggs\", {'tags': ['N', 'V', 'J', 'N']}),\n",
    "    (\"Eat blue ham\", {'tags': ['V', 'J', 'N']})\n",
    "]\n",
    "\n",
    "# We set up some annotations for the language, output directory, and a number of training iterations.\n",
    "\n",
    "\n",
    "plac.annotations(\n",
    "    lang=(\"ISO Code of language to use\", \"option\", \"l\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "\n",
    "\n",
    "def main(lang='en', output_dir=None, n_iter=25):\n",
    "    \"\"\"Create a new model, set up the pipeline and train the tagger. In order to\n",
    "    train the tagger with a custom tag map, we're creating a new Language\n",
    "    instance with a custom vocab.\n",
    "    \"\"\"\n",
    "    nlp = spacy.blank(lang)\n",
    "    # add the tagger to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    tagger = nlp.create_pipe('tagger')\n",
    "    # Add the tags. This needs to be done before you start training.\n",
    "    for tag, values in TAG_MAP.items():\n",
    "        tagger.add_label(tag, values)\n",
    "    nlp.add_pipe(tagger)\n",
    "\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            nlp.update([text], [annotations], sgd=optimizer, losses=losses)\n",
    "        print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = \"I like blue eggs\"\n",
    "    doc = nlp(test_text)\n",
    "    print('Tags', [(t.text, t.tag_, t.pos_) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the save model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc = nlp2(test_text)\n",
    "        print('Tags', [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [lang] [output_dir] [n_iter]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    plac.call(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [lang] [output_dir] [n_iter]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda3/envs/nlp/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "plac.call(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
