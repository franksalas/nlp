{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy's Language Models                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic preprocessing with language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('This is a sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speach (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Jon', 'PROPN')\n",
      "('and', 'CCONJ')\n",
      "('I', 'PRON')\n",
      "('wne', 'VERB')\n",
      "('tto', 'NOUN')\n",
      "('the', 'DET')\n",
      "('park', 'NOUN')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Jon and I wne tto the park')\n",
    "\n",
    "for token in doc:\n",
    "    print((token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name entity recofniton\n",
    "- real world object that is assinged a name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft 0 9 ORG\n",
      "Europe 31 37 LOC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Microsoft has offices all over Europe')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has the following built-in entity \n",
    "- types:PERSON: People, including fictional ones\n",
    "- NORP: Nationalities or religious or political groups\n",
    "- FACILITY: Buildings, airports, highways, bridges, and so on\n",
    "- ORG: Companies, agencies, institutions, and so on\n",
    "- GPE: Countries, cities, and states\n",
    "- LOC: Non GPE locations, mountain ranges, and bodies of water\n",
    "- PRODUCT: Objects, vehicles, foods, and so on (not services)\n",
    "- EVENT: Named hurricanes, battles, wars, sports events, and so on\n",
    "- WORK_OF_ART: Titles of books, songs, and so onLAW: Named documents made into lawsLANGUAGE: Any named language\n",
    "\n",
    "## Rule based matching\n",
    "\n",
    "- ORTH: The exact verbatim text of a token\n",
    "- LOWER, UPPER: The lowercase and uppercase form of the token\n",
    "- IS_ALPHA: Token text consists of alphanumeric chars\n",
    "- IS_ASCII: Token text consists of ASCII characters\n",
    "- IS_DIGIT: Token text consists of digits\n",
    "- IS_LOWER, IS_UPPER, IS_TITLE: Token text is in lowercase, uppercase, and title\n",
    "- IS_PUNCT, IS_SPACE, IS_STOP: Token is punctuation, whitespace, and a stop word\n",
    "- LIKE_NUM, LIKE_URL, LIKE_EMAIL: Token text resembles a number, URL, and email\n",
    "- POS, TAG: The token's simple and extended POS tag\n",
    "- DEP, LEMMA, SHAPE: The token's dependency label, lemma, and shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- get rid of info that it wont be used\n",
    "- in text mining and nlp they are called **stop words**\n",
    "- ex: *of, the, want, to, have,..*\n",
    "- identify as `IS_STOP` attribute\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can add our own stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [u'say', u'be', u'said', u'says', u'saying', 'field']\n",
    "for stopword in my_stop_words:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'that', 'although', 'twenty', 'your', 'get', 'elsewhere', 'make', 'put', 'most', 'name', 'whatever', 'thereby', 'through', 'against', 'whereafter', 'next', 'an', 'before', 'but', 'to', 'with', 'other', 'herein', 'namely', 'itself', 'thus', 'another', 'cannot', 'if', 'me', 'who', 'both', 'yourselves', 'for', 'as', 'by', 'everywhere', 'might', 'beyond', 'ca', 'too', 'besides', 'seem', 'almost', 'upon', 'due', 'sometimes', 'somewhere', 'perhaps', 'various', 'last', 'towards', 'beside', 'also', 'am', 'him', 'throughout', 'anyway', 'my', 'go', 'there', 'top', 'already', 'of', 'however', 'mostly', 'because', 'himself', 'about', 'its', 'two', 'can', 'us', 'seems', 'when', 'wherever', 'sixty', 'under', 'call', 'on', 'yet', 'hers', 'yours', 'behind', 'or', 'former', 'while', 'not', 'somehow', 'thence', 'really', 'several', 'thereupon', 'was', 'whenever', 'hereafter', 'never', 'will', 'just', 'serious', 'whose', 'yourself', 'still', 'along', 'fifteen', 'otherwise', 'using', 'his', 'after', 'hence', 'their', 'therefore', 're', 'whole', 'being', 'least', 'during', 'it', 'bottom', 'here', 'others', 'eleven', 'ten', 'be', 'each', 'latter', 'now', 'regarding', 'nevertheless', 'doing', 'used', 'across', 'may', 'he', 'neither', 'once', 'per', 'whither', 'hereby', 'must', 'in', 'hundred', 'no', 'below', 'either', 'without', 'within', 'again', 'did', 'made', 'give', 'have', 'anywhere', 'above', 'seeming', 'ours', 'take', 'what', 'any', 'afterwards', 'five', 'had', 'over', 'noone', 'which', 'toward', 'would', 'nowhere', 'much', 'beforehand', 'between', 'has', 'first', 'how', 'back', 'i', 'latterly', 'part', 'why', 'everything', 'moreover', 'anything', 'nobody', 'formerly', 'these', 'she', 'amongst', 'more', 'became', 'own', 'quite', 'whereby', 'become', 'becoming', 'those', 'fifty', 'forty', 'up', 'none', 'so', 'see', 'does', 'them', 'were', 'around', 'onto', 'should', 'out', 'same', 'always', 'three', 'full', 'twelve', 'off', 'whoever', 'into', 'enough', 'whether', 'please', 'they', 'indeed', 'thru', 'some', 'the', 'four', 'been', 'unless', 'show', 'third', 'ourselves', 'empty', 'we', 'hereupon', 'every', 'even', 'move', 'myself', 'then', 'this', 'wherein', 'few', 'are', 'six', 'all', 'nor', 'than', 'amount', 'nine', 'until', 'alone', 'her', 'whereas', 'though', 'among', 'everyone', 'nothing', 'themselves', 'together', 'something', 'from', 'at', 'via', 'sometime', 'a', 'very', 'seemed', 'since', 'becomes', 'anyone', 'could', 'well', 'therein', 'our', 'done', 'further', 'meanwhile', 'eight', 'many', 'rather', 'thereafter', 'down', 'side', 'you', 'often', 'less', 'only', 'else', 'whom', 'front', 'whereupon', 'is', 'keep', 'where', 'anyhow', 'one', 'say', 'do', 'someone', 'such', 'except', 'and', 'herself', 'mine', 'whence', 'ever'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# add to STOP_WORDS\n",
    "\n",
    "STOP_WORDS.add('your_word_list')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horse', 'gallop', 'past', 'river']\n"
     ]
    }
   ],
   "source": [
    "# Example: Clean up a sentence\n",
    "\n",
    "doc = nlp(u'the horse galloped down the field and past the river.')\n",
    "sentence = []\n",
    "\n",
    "for w in doc:\n",
    "  # if it's not a stop word or punctuation mark, add it to our article!\n",
    "  if w.text != 'n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "    # we add the lematized version of the word\n",
    "    sentence.append(w.lemma_)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using `.is_stop` `is_punct` and `.like_num` attributes removes parts of the sentece we dont need.\n",
    "- the lemmatized form of the word can be acees through `.lemma_`\n",
    "- can remove words base on use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary         \n",
    "\n",
    "spaCy offers us an easy way to annotate your text data very easily, and with the language model, we annotate your text data with a lot of information – not just tokenizing and whether it is a stop word or not, but also the part of speech, named entity tag, and so on – we can also train these annotating models on our own, giving a lot of power to the language model and processing pipeline! Downloading the models and using virtual environments are also an important part of this process. We will now move on to using our cleaned data in a way that machines can understand us – with vectors, and what kind of Python libraries we would need for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
